# Tree-based Speculative Decoding å®éªŒæŠ¥å‘Š

## ğŸ“‹ æ¦‚è¿°

æœ¬æŠ¥å‘Šè®°å½•äº† Tree-based Speculative Decoding (æ ‘å½¢æŠ•æœºè§£ç ) çš„å®éªŒè¿‡ç¨‹å’Œç»“æœã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æœ€ä¼˜å‚æ•°é…ç½®ä¸‹ï¼ŒTree V2 æ–¹æ³•å®ç°äº† **1.62x åŠ é€Ÿæ¯”**ï¼Œæ˜¾è‘—ä¼˜äº HuggingFace åŸç”Ÿ Assisted Generation (1.36x) å’Œ Linear Speculative Decoding (1.11x)ã€‚

---

## 1. å®éªŒç¯å¢ƒ

### 1.1 ç¡¬ä»¶é…ç½®

| é¡¹ç›® | é…ç½® |
|------|------|
| GPU | NVIDIA GPU (CUDA) |
| æ˜¾å­˜ | è¶³å¤Ÿè¿è¡Œ Pythia-2.8B + Pythia-70M |
| ç³»ç»Ÿ | Linux 5.15.0-126-generic |

### 1.2 è½¯ä»¶ç¯å¢ƒ

| é¡¹ç›® | ç‰ˆæœ¬ |
|------|------|
| Python | 3.x |
| PyTorch | 2.0+ (æ”¯æŒ torch.compile) |
| Transformers | 4.x (æ”¯æŒ DynamicCache) |
| CUDA | å…¼å®¹ç‰ˆæœ¬ |

### 1.3 æ¨¡å‹é…ç½®

| æ¨¡å‹è§’è‰² | æ¨¡å‹åç§° | å‚æ•°é‡ | è·¯å¾„ |
|---------|---------|--------|------|
| Target Model | Pythia-2.8B | 2.8B | `/mnt/disk1/models/pythia-2.8b` |
| Draft Model | Pythia-70M | 70M | `/mnt/disk1/models/pythia-70m` |

---

## 2. å®éªŒæ–¹æ³•

### 2.1 æµ‹è¯•æ–¹æ³•åˆ—è¡¨

æœ¬å®éªŒå¯¹æ¯”äº†ä»¥ä¸‹ 7 ç§æ¨ç†æ–¹æ³•ï¼š

1. **Baseline (AR)** - çº¯è‡ªå›å½’ç”Ÿæˆï¼Œä½œä¸ºåŸºå‡†
2. **HuggingFace Assisted Generation** - HuggingFace å®˜æ–¹å®ç°çš„è¾…åŠ©ç”Ÿæˆ
3. **Linear Speculative Decoding** - çº¿æ€§æŠ•æœºè§£ç  (K=5,6,7,8)
4. **Tree V2 Speculative Decoding** - æ ‘å½¢æŠ•æœºè§£ç  V2 ç‰ˆæœ¬
5. **StreamingLLM + Spec Decode** - ç»“åˆ StreamingLLM çš„æŠ•æœºè§£ç 

### 2.2 Tree-based Speculative Decoding åŸç†

Tree-based Speculative Decoding çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

```
ä¼ ç»Ÿ Linear æ–¹æ³•:
  Draft æ¨¡å‹ç”Ÿæˆ: t1 -> t2 -> t3 -> t4 -> t5 (çº¿æ€§åºåˆ—)
  Target éªŒè¯: é€ä¸ªéªŒè¯

Tree-based æ–¹æ³•:
  Draft æ¨¡å‹ç”Ÿæˆæ ‘å½¢ç»“æ„:
                    t1
                 /  |  \
               t2a t2b t2c
              / |   |   | \
           t3a t3b t3c t3d t3e
           ...

  Target ä¸€æ¬¡éªŒè¯æ•´æ£µæ ‘çš„æ‰€æœ‰åˆ†æ”¯
```

**ä¼˜åŠ¿ï¼š**
- å¹¶è¡ŒéªŒè¯å¤šä¸ªå€™é€‰è·¯å¾„
- æé«˜æ‰¾åˆ°æ­£ç¡® token åºåˆ—çš„æ¦‚ç‡
- æ›´å……åˆ†åˆ©ç”¨ GPU å¹¶è¡Œè®¡ç®—èƒ½åŠ›

### 2.3 å…³é”®å‚æ•°è¯´æ˜

| å‚æ•° | å«ä¹‰ | æœ€ä¼˜å€¼ |
|------|------|--------|
| **D (tree_depth)** | æ ‘çš„æœ€å¤§æ·±åº¦ | 8 |
| **B (branch_factor)** | æ¯ä¸ªèŠ‚ç‚¹çš„åˆ†æ”¯æ•° | 3 |
| **t (probability_threshold)** | æ¦‚ç‡å‰ªæé˜ˆå€¼ | 0.03 |

---

## 3. å®éªŒé…ç½®

### 3.1 å‚æ•°æœç´¢é…ç½®

```python
# å‚æ•°æœç´¢èŒƒå›´
depths = [3, 4, 5, 6, 7, 8]      # æ ‘æ·±åº¦
branches = [2, 3, 4]             # åˆ†æ”¯å› å­
thresholds = [0.01, 0.02, 0.03, 0.05, 0.1]  # æ¦‚ç‡é˜ˆå€¼
token_lengths = [100, 200, 300, 500, 1000]  # ç”Ÿæˆé•¿åº¦

# æ€»é…ç½®æ•°: 6 Ã— 3 Ã— 5 Ã— 5 = 450 ç§ç»„åˆ
```

### 3.2 æ€§èƒ½æµ‹è¯•é…ç½®

```python
MAX_NEW_TOKENS = 500        # ç”Ÿæˆ token æ•°
NUM_RUNS = 5                # æ¯ä¸ªæ–¹æ³•è¿è¡Œæ¬¡æ•°
SKIP_FIRST = True           # è·³è¿‡é¦–æ¬¡ warmup
WARMUP_ROUNDS = 10          # é¢„çƒ­è½®æ•°

# æµ‹è¯• prompt
PROMPT = """Write a detailed technical explanation about the development 
of large language models. Cover the history, architecture innovations, 
training techniques, and future directions..."""
```

### 3.3 æœ€ä¼˜ Tree V2 é…ç½®

```python
TREE_DEPTH = 8              # æ ‘æ·±åº¦
TREE_BRANCH = 3             # åˆ†æ”¯å› å­  
TREE_THRESHOLD = 0.03       # æ¦‚ç‡é˜ˆå€¼
MAX_TREE_NODES = 128        # æœ€å¤§æ ‘èŠ‚ç‚¹æ•°
```

---

## 4. å®éªŒè„šæœ¬

### 4.1 å‚æ•°æœç´¢è„šæœ¬

**è·¯å¾„**: `papers/tree_param_search.py`

```python
# æ ¸å¿ƒæœç´¢é€»è¾‘
for depth in depths:
    for branch in branches:
        for threshold in thresholds:
            for tokens in token_lengths:
                # åˆ›å»º Tree V2 ç”Ÿæˆå™¨
                gen = TreeSpeculativeGeneratorV2(
                    target_model, draft_model, tokenizer,
                    tree_depth=depth,
                    branch_factor=branch,
                    probability_threshold=threshold,
                    max_tree_nodes=128
                )
                
                # æµ‹é‡æ€§èƒ½
                throughput, stats = measure_performance(gen, tokens)
                
                # è®°å½•ç»“æœ
                results.append({
                    'depth': depth,
                    'branch': branch,
                    'threshold': threshold,
                    'tokens': tokens,
                    'throughput': throughput,
                    'speedup': throughput / baseline
                })
```

**è¿è¡Œå‘½ä»¤**:
```bash
cd /mnt/disk1/ljm/LLM-Efficient-Reasoning
python papers/tree_param_search.py
```

### 4.2 æ€§èƒ½å¯¹æ¯”è„šæœ¬

**è·¯å¾„**: `papers/benchmark_optimal_config.py`

```python
# æµ‹è¯•æ‰€æœ‰æ–¹æ³•
methods = [
    ("Baseline", run_baseline),
    ("HF Assisted", run_hf_assisted),
    ("Linear K=5", run_linear_k5),
    ("Linear K=6", run_linear_k6),
    ("Linear K=7", run_linear_k7),
    ("Linear K=8", run_linear_k8),
    ("Tree V2 D=8 B=3 t=0.03", run_tree_v2),
    ("Streaming K=6 cache=512", run_streaming_512),
    ("Streaming K=6 cache=1024", run_streaming_1024),
]

for name, run_fn in methods:
    results = []
    for i in range(NUM_RUNS):
        cleanup()
        torch.cuda.synchronize()
        start = time.perf_counter()
        tokens, stats = run_fn()
        elapsed = time.perf_counter() - start
        throughput = tokens / elapsed
        if i > 0:  # è·³è¿‡é¦–æ¬¡ warmup
            results.append(throughput)
    
    avg_throughput = sum(results) / len(results)
    print(f"{name}: {avg_throughput:.1f} t/s")
```

**è¿è¡Œå‘½ä»¤**:
```bash
cd /mnt/disk1/ljm/LLM-Efficient-Reasoning
python papers/benchmark_optimal_config.py
```

### 4.3 ç»“æœåˆ†æè„šæœ¬

**è·¯å¾„**: `papers/analyze_tree_search_results.py`

```python
# åˆ†æå‚æ•°æœç´¢ç»“æœ
def analyze_results(json_path):
    with open(json_path, 'r') as f:
        data = json.load(f)
    
    results = data['results']
    
    # æŒ‰åŠ é€Ÿæ¯”æ’åº
    sorted_results = sorted(results, key=lambda x: x['speedup'], reverse=True)
    
    # è¾“å‡º Top 10
    print("Top 10 é…ç½®:")
    for i, r in enumerate(sorted_results[:10]):
        print(f"{i+1}. D={r['depth']} B={r['branch']} t={r['threshold']}")
        print(f"   {r['speedup']:.2f}x ({r['throughput']:.1f} t/s)")
```

**è¿è¡Œå‘½ä»¤**:
```bash
cd /mnt/disk1/ljm/LLM-Efficient-Reasoning
python papers/analyze_tree_search_results.py results/tree_param_search_20251231_140952.json
```

---

## 5. å®éªŒç»“æœ

### 5.1 å‚æ•°æœç´¢ç»“æœ

å‚æ•°æœç´¢å…±æµ‹è¯•äº† 450 ç§é…ç½®ç»„åˆï¼Œç»“æœä¿å­˜åœ¨:
`results/tree_param_search_20251231_140952.json`

#### 5.1.1 Top 10 æœ€ä¼˜é…ç½®

| æ’å | Tokens | D | B | t | ååé‡ | Baseline | åŠ é€Ÿæ¯” |
|-----|--------|---|---|------|--------|----------|--------|
| 1 | 500 | 8 | 3 | 0.03 | 221.4 t/s | 123.9 t/s | **1.79x** |
| 2 | 500 | 7 | 3 | 0.03 | 217.4 t/s | 123.9 t/s | 1.76x |
| 3 | 500 | 8 | 3 | 0.02 | 217.2 t/s | 123.9 t/s | 1.75x |
| 4 | 500 | 8 | 4 | 0.02 | 212.5 t/s | 123.9 t/s | 1.72x |
| 5 | 500 | 6 | 3 | 0.03 | 212.1 t/s | 123.9 t/s | 1.71x |
| 6 | 1000 | 6 | 3 | 0.05 | 212.3 t/s | 124.5 t/s | 1.71x |
| 7 | 1000 | 6 | 3 | 0.10 | 211.2 t/s | 124.5 t/s | 1.70x |
| 8 | 500 | 7 | 3 | 0.02 | 209.5 t/s | 123.9 t/s | 1.69x |
| 9 | 1000 | 7 | 3 | 0.10 | 210.0 t/s | 124.5 t/s | 1.69x |
| 10 | 1000 | 8 | 2 | 0.10 | 208.7 t/s | 124.5 t/s | 1.68x |

#### 5.1.2 å„ Token é•¿åº¦æœ€ä¼˜é…ç½®

| Token é•¿åº¦ | æœ€ä¼˜é…ç½® | ååé‡ | åŠ é€Ÿæ¯” |
|-----------|----------|--------|--------|
| 100 | D=7, B=3, t=0.03 | 150.7 t/s | 1.43x |
| 200 | D=7, B=3, t=0.03 | 193.2 t/s | 1.54x |
| 300 | D=7, B=3, t=0.03 | 199.0 t/s | 1.60x |
| **500** | **D=8, B=3, t=0.03** | **221.4 t/s** | **1.79x** |
| 1000 | D=6, B=3, t=0.05 | 212.3 t/s | 1.71x |

#### 5.1.3 å‚æ•°æ•æ„Ÿæ€§åˆ†æ

**Branch Factor (B) çš„å½±å“:**
| B | å¹³å‡åŠ é€Ÿæ¯” | æœ€å¤§åŠ é€Ÿæ¯” |
|---|-----------|-----------|
| 2 | 1.11x | 1.68x |
| **3** | **1.31x** | **1.79x** |
| 4 | 1.19x | 1.72x |

**ç»“è®º**: B=3 æ˜¯æœ€ä¼˜åˆ†æ”¯å› å­

**Probability Threshold (t) çš„å½±å“:**
| t | å¹³å‡åŠ é€Ÿæ¯” | æœ€å¤§åŠ é€Ÿæ¯” |
|---|-----------|-----------|
| 0.01 | 1.09x | 1.57x |
| 0.02 | 1.28x | 1.75x |
| **0.03** | **1.31x** | **1.79x** |
| 0.05 | 1.17x | 1.71x |
| 0.10 | 1.19x | 1.70x |

**ç»“è®º**: t=0.03 æ˜¯æœ€ä¼˜é˜ˆå€¼

### 5.2 æ€§èƒ½å¯¹æ¯”ç»“æœ

åœ¨ 500 tokensã€æœ€ä¼˜é…ç½® (D=8, B=3, t=0.03) ä¸‹çš„æ€§èƒ½å¯¹æ¯”ï¼š

| æ’å | æ–¹æ³• | ååé‡ | åŠ é€Ÿæ¯” | å¤‡æ³¨ |
|-----|------|--------|--------|------|
| ğŸ¥‡ | **Tree V2 (D=8, B=3, t=0.03)** | **193.4 t/s** | **1.62x** | æœ€ä¼˜ |
| ğŸ¥ˆ | HuggingFace Assisted | 161.9 t/s | 1.36x | å®˜æ–¹å®ç° |
| ğŸ¥‰ | Linear K=6 | 133.1 t/s | 1.11x | è‡ªå®šä¹‰å®ç° |
| 4 | Streaming K=6 cache=1024 | 132.9 t/s | 1.11x | StreamingLLM |
| 5 | Linear K=7 | 131.9 t/s | 1.10x | |
| 6 | Linear K=8 | 128.9 t/s | 1.08x | |
| 7 | Linear K=5 | 125.2 t/s | 1.05x | |
| 8 | Baseline (AR) | 119.4 t/s | 1.00x | åŸºå‡† |
| 9 | Streaming K=6 cache=512 | 114.2 t/s | 0.96x | å¼€é”€è¿‡å¤§ |

### 5.3 è¯¦ç»†è¿è¡Œæ•°æ®

#### Tree V2 (D=8, B=3, t=0.03)
```
Run 1: 500 tokens, 2.83s, 176.8 t/s (warmup, è·³è¿‡)
Run 2: 500 tokens, 2.57s, 194.5 t/s
Run 3: 500 tokens, 2.59s, 192.9 t/s
Run 4: 500 tokens, 2.60s, 192.3 t/s
Run 5: 500 tokens, 2.58s, 194.1 t/s
>>> å¹³å‡: 193.4 t/s (1.62x)
    æ¥å—ç‡: 29.6%
```

#### HuggingFace Assisted
```
Run 1: 500 tokens, 3.16s, 158.2 t/s (warmup, è·³è¿‡)
Run 2: 500 tokens, 3.08s, 162.3 t/s
Run 3: 500 tokens, 3.10s, 161.2 t/s
Run 4: 500 tokens, 3.10s, 161.4 t/s
Run 5: 500 tokens, 3.08s, 162.6 t/s
>>> å¹³å‡: 161.9 t/s (1.36x)
```

#### Linear K=6
```
Run 1: 500 tokens, 4.04s, 123.9 t/s (warmup, è·³è¿‡)
Run 2: 500 tokens, 3.76s, 132.8 t/s
Run 3: 500 tokens, 3.76s, 133.0 t/s
Run 4: 500 tokens, 3.75s, 133.2 t/s
Run 5: 500 tokens, 3.76s, 133.1 t/s
>>> å¹³å‡: 133.1 t/s (1.11x)
    æ¥å—ç‡: 68.3%, æ¯è½® tokens: 4.10
```

---

## 6. ç»“è®ºä¸åˆ†æ

### 6.1 æ ¸å¿ƒå‘ç°

1. **Tree V2 æ˜¯æœ€å¿«çš„æ–¹æ³•**
   - å®ç°äº† **1.62x åŠ é€Ÿæ¯”** (193.4 t/s)
   - æ¯” HuggingFace å®˜æ–¹å®ç°å¿« **19%**
   - æ¯” Linear æ–¹æ³•å¿« **45%**

2. **æœ€ä¼˜å‚æ•°é…ç½®**
   - æ ‘æ·±åº¦ D = 8
   - åˆ†æ”¯å› å­ B = 3
   - æ¦‚ç‡é˜ˆå€¼ t = 0.03
   - ç”Ÿæˆé•¿åº¦ = 500 tokens

3. **Tree V2 çš„ä¼˜åŠ¿æ¥æº**
   - å¹¶è¡ŒéªŒè¯å¤šä¸ªå€™é€‰åˆ†æ”¯
   - æ¦‚ç‡å‰ªæå‡å°‘æ— æ•ˆè®¡ç®—
   - æ›´é«˜æ•ˆåˆ©ç”¨ GPU å¹¶è¡Œèƒ½åŠ›

### 6.2 æ–¹æ³•å¯¹æ¯”æ€»ç»“

| æ–¹æ³• | åŠ é€Ÿæ¯” | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|--------|------|------|
| **Tree V2** | **1.62x** | æœ€å¿«ã€å¯å®šåˆ¶ | å®ç°å¤æ‚ |
| HF Assisted | 1.36x | å®˜æ–¹æ”¯æŒã€ç¨³å®š | ä¸å¯å®šåˆ¶ |
| Linear | 1.11x | ç®€å•ã€æ˜“ç†è§£ | åŠ é€Ÿæœ‰é™ |
| Streaming | 1.11x | æ”¯æŒé•¿åºåˆ— | çŸ­åºåˆ—å¼€é”€å¤§ |

### 6.3 é€‚ç”¨åœºæ™¯å»ºè®®

| åœºæ™¯ | æ¨èæ–¹æ³• | ç†ç”± |
|------|----------|------|
| è¿½æ±‚æœ€å¤§é€Ÿåº¦ | Tree V2 (D=8, B=3, t=0.03) | åŠ é€Ÿæ¯”æœ€é«˜ |
| ç”Ÿäº§ç¯å¢ƒç¨³å®šæ€§ | HuggingFace Assisted | å®˜æ–¹ç»´æŠ¤ã€ç¨³å®š |
| è¶…é•¿åºåˆ—ç”Ÿæˆ | Streaming + Spec Decode | å†…å­˜æ•ˆç‡é«˜ |
| å¿«é€ŸåŸå‹éªŒè¯ | Linear K=6 | å®ç°ç®€å• |

### 6.4 æœªæ¥ä¼˜åŒ–æ–¹å‘

1. **Tree + StreamingLLM èåˆ** - ç»“åˆä¸¤è€…ä¼˜åŠ¿ç”¨äºè¶…é•¿åºåˆ—
2. **åŠ¨æ€å‚æ•°è°ƒæ•´** - æ ¹æ®ç”Ÿæˆå†…å®¹åŠ¨æ€è°ƒæ•´æ ‘ç»“æ„
3. **torch.compile ä¼˜åŒ–** - åˆ©ç”¨ PyTorch 2.0 ç¼–è¯‘åŠ é€Ÿ
4. **æ›´å¤§æ¨¡å‹éªŒè¯** - åœ¨ 7Bã€13B æ¨¡å‹ä¸ŠéªŒè¯æ•ˆæœ

---

## 7. å¤ç°æŒ‡å—

### 7.1 ç¯å¢ƒå‡†å¤‡

```bash
# å…‹éš†ä»“åº“
git clone <repository_url>
cd LLM-Efficient-Reasoning

# å®‰è£…ä¾èµ–
pip install torch transformers matplotlib numpy
```

### 7.2 è¿è¡Œå‚æ•°æœç´¢

```bash
# å®Œæ•´å‚æ•°æœç´¢ (çº¦ 2-3 å°æ—¶)
python papers/tree_param_search.py

# ç»“æœä¿å­˜åœ¨ results/tree_param_search_*.json
```

### 7.3 è¿è¡Œæ€§èƒ½å¯¹æ¯”

```bash
# åœ¨æœ€ä¼˜é…ç½®ä¸‹å¯¹æ¯”æ‰€æœ‰æ–¹æ³• (çº¦ 10 åˆ†é’Ÿ)
python papers/benchmark_optimal_config.py
```

### 7.4 åˆ†æç»“æœ

```bash
# åˆ†æå‚æ•°æœç´¢ç»“æœ
python papers/analyze_tree_search_results.py results/tree_param_search_20251231_140952.json
```

---

## 8. é™„å½•

### 8.1 ç›¸å…³æ–‡ä»¶åˆ—è¡¨

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `spec_decode/core/token_tree.py` | TokenTree æ•°æ®ç»“æ„å®ç° |
| `spec_decode/core/tree_speculative_generator.py` | Tree V2 ç”Ÿæˆå™¨å®ç° |
| `papers/tree_param_search.py` | å‚æ•°æœç´¢è„šæœ¬ |
| `papers/benchmark_optimal_config.py` | æ€§èƒ½å¯¹æ¯”è„šæœ¬ |
| `papers/analyze_tree_search_results.py` | ç»“æœåˆ†æè„šæœ¬ |
| `results/tree_param_search_20251231_140952.json` | å‚æ•°æœç´¢åŸå§‹æ•°æ® |

### 8.2 å‚è€ƒæ–‡çŒ®

1. Leviathan et al., "Fast Inference from Transformers via Speculative Decoding", ICML 2023
2. Miao et al., "SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference", 2024
3. Xiao et al., "Efficient Streaming Language Models with Attention Sinks", ICLR 2024

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2026å¹´1æœˆ2æ—¥  
**å®éªŒç¯å¢ƒ**: Pythia-2.8B + Pythia-70M on CUDA

