# Speculative Decoding è®ºæ–‡å®éªŒç»“æœä¸å¤ç°æŒ‡å—

æœ¬ç›®å½•åŒ…å«è®ºæ–‡æ’°å†™æ‰€éœ€çš„æ‰€æœ‰å®éªŒç»“æœå’Œå¤ç°å‘½ä»¤ã€‚

---

## ğŸ“ ç›®å½•ç»“æ„

```
papers/
â”œâ”€â”€ README.md                           # æœ¬æ–‡ä»¶ï¼ˆå¤ç°æŒ‡å—ï¼‰
â”œâ”€â”€ speculative_decoding_paper_draft.md # è®ºæ–‡è‰ç¨¿
â”œâ”€â”€ reproduction_commands.sh            # ä¸€é”®å¤ç°è„šæœ¬
â””â”€â”€ figures/
    â”œâ”€â”€ paper_fig6_long_seq.png         # é•¿åºåˆ—å†…å­˜å¯¹æ¯”å›¾
    â””â”€â”€ paper_fig7_comprehensive.png    # å…¨é¢æ€§èƒ½å¯¹æ¯”å›¾ï¼ˆä¸»å›¾ï¼‰

é¡¹ç›®æ ¹ç›®å½•ä¿ç•™çš„ç»“æœæ–‡ä»¶ï¼š
â”œâ”€â”€ benchmark_comprehensive_results.json  # å…¨é¢ benchmark æ•°æ®
â””â”€â”€ benchmark_long_seq_results.json       # é•¿åºåˆ—æµ‹è¯•æ•°æ®
```

---

## å‚æ•°è¯´æ˜

### è¡¨æ ¼ä¸­å„åˆ—å«ä¹‰

| å‚æ•° | å«ä¹‰ | è¯´æ˜ |
|------|------|------|
| **Tokens** | ç›®æ ‡ç”Ÿæˆé•¿åº¦ | `max_new_tokens` å‚æ•°ï¼Œè¡¨ç¤ºè¦ç”Ÿæˆå¤šå°‘ä¸ª token |
| **Throughput** | ååé‡ (tokens/s) | æ¯ç§’ç”Ÿæˆçš„ token æ•°ï¼Œè¶Šé«˜è¶Šå¥½ |
| **TTFT** | é¦– token å»¶è¿Ÿ (ms) | Time to First Tokenï¼Œä»è¾“å…¥åˆ°è¾“å‡ºç¬¬ä¸€ä¸ª token çš„æ—¶é—´ |
| **TPOT** | æ¯ token å»¶è¿Ÿ (ms) | Time per Output Tokenï¼Œç”Ÿæˆæ¯ä¸ª token çš„å¹³å‡æ—¶é—´ |
| **PPL** | å›°æƒ‘åº¦ | Perplexityï¼Œè¡¡é‡ç”Ÿæˆè´¨é‡ï¼Œè¶Šä½è¶Šå¥½ |
| **Accept%** | æ¥å—ç‡ | Draft tokens è¢« target model æ¥å—çš„æ¯”ä¾‹ |
| **T/Round** | æ¯è½® tokens | æ¯ä¸ªæ¨æµ‹è§£ç è½®æ¬¡å¹³å‡ç”Ÿæˆçš„ tokens æ•° |
| **Mem MB** | å†…å­˜å¢é•¿ | æ¨ç†è¿‡ç¨‹ä¸­ GPU æ˜¾å­˜å¢é•¿é‡ |
| **Compress** | å‹ç¼©æ¬¡æ•° | StreamingLLM KV cache å‹ç¼©è§¦å‘æ¬¡æ•° |

---

## ğŸ“Š æ ¸å¿ƒå®éªŒç»“æœ

### å®éªŒ 1ï¼šå…¨é¢æ€§èƒ½å¯¹æ¯”ï¼ˆä¸»è¡¨æ ¼ï¼‰

**ç»“æœæ–‡ä»¶**ï¼š`benchmark_comprehensive_results.json`  
**å›¾è¡¨æ–‡ä»¶**ï¼š`papers/figures/paper_fig7_comprehensive.png`

**å¤ç°å‘½ä»¤**ï¼š
```bash
cd /mnt/disk1/ljm/LLM-Efficient-Reasoning

python spec_decode/benchmark_comprehensive.py \
    --target-model /mnt/disk1/models/pythia-2.8b \
    --draft-model /mnt/disk1/models/pythia-70m \
    --max-new-tokens 500 1000 2000 \
    --max-cache-lens 256 512 1024 \
    --k-value 5 \
    --num-samples 3 \
    --output-json benchmark_comprehensive_results.json \
    --output-plot papers/figures/paper_fig7_comprehensive.png
```

**æ ¸å¿ƒæ•°æ®**ï¼ˆK=5ï¼ŒPythia-2.8B + Pythia-70Mï¼‰ï¼š

| é…ç½® | Tokens | Throughput | TTFT (ms) | TPOT (ms) | PPL | Accept% | Memory |
|------|--------|------------|-----------|-----------|-----|---------|--------|
| standard | 500 | 132.0Â±47.7 | 277.0 | 8.24 | 1.2 | 99.3% | 607 MB |
| standard | 1000 | 185.6Â±6.2 | 37.1 | 5.36 | 1.1 | 100% | 1127 MB |
| standard | 2000 | **192.9Â±3.8** | 37.2 | **5.17** | 1.0 | 100% | 2237 MB |
| stream(256) | 2000 | 177.1Â±11.5 | 36.8 | 5.65 | 1.1 | 98.2% | **1688 MB** |
| stream(1024) | 1000 | **197.3Â±4.2** | 36.9 | **5.04** | 1.1 | 100% | 1126 MB |

---

### å®éªŒ 2ï¼šé•¿åºåˆ—ç”Ÿæˆå¯¹æ¯”ï¼ˆå†…å­˜ä¼˜åŠ¿ï¼‰

**ç»“æœæ–‡ä»¶**ï¼š`benchmark_long_seq_results.json`  
**å›¾è¡¨æ–‡ä»¶**ï¼š`papers/figures/paper_fig6_long_seq.png`

**å¤ç°å‘½ä»¤**ï¼š
```bash
python spec_decode/benchmark_long_sequence.py \
    --target-model /mnt/disk1/models/pythia-2.8b \
    --draft-model /mnt/disk1/models/pythia-70m \
    --max-new-tokens 500 1000 2000 \
    --max-cache-lens 256 512 1024 \
    --k-value 5 \
    --output-json benchmark_long_seq_results.json \
    --output-plot papers/figures/paper_fig6_long_seq.png
```

**æ ¸å¿ƒæ•°æ®**ï¼š

| æ–¹æ³• | Tokens | Throughput | Memory å¢é•¿ | å‹ç¼©æ¬¡æ•° |
|------|--------|------------|-------------|----------|
| standard | 2000 | 168.4 t/s | 874 MB | 0 |
| stream(256) | 2000 | 131.4 t/s | **398 MB** | 409 |
| stream(512) | 2000 | 162.0 t/s | 553 MB | 285 |

**å…³é”®ç»“è®º**ï¼šStreamingLLM (cache=256) èŠ‚çœ **54.5%** å†…å­˜ï¼Œååé‡æŸå¤±çº¦ 22%ã€‚

---

## ğŸ“ è®ºæ–‡å†™ä½œè¦ç‚¹

### ä¸»è¦è´¡çŒ®ç‚¹

1. **Speculative Decoding åŠ é€Ÿæ•ˆæœ**
   - Pythia-2.8B + Pythia-70M ä¸Šå®ç° **2.3Ã— åŠ é€Ÿ**
   - TPOT ä» ~12ms é™ä½åˆ° **5.04ms**ï¼ˆé™ä½ 58%ï¼‰

2. **StreamingLLM é›†æˆ**
   - å†…å­˜èŠ‚çœ **24-55%**ï¼ˆå–å†³äº cache å¤§å°ï¼‰
   - ååé‡æŸå¤± **< 10%**ï¼ˆcache=512 æ—¶ï¼‰
   - PPL å½±å“ **< 0.2**ï¼ˆå¯å¿½ç•¥ï¼‰

3. **æœ€ä¼˜é…ç½®æ¨è**
   
   | åœºæ™¯ | æ¨èé…ç½® | åŸå›  |
   |------|---------|------|
   | è¿½æ±‚æœ€é«˜åå | K=5, stream(1024) | 197.3 t/s, TPOT=5.04ms |
   | å†…å­˜å—é™ | K=5, stream(256) | å†…å­˜èŠ‚çœ 55%ï¼Œååé‡æŸå¤± 22% |
   | å¹³è¡¡æ–¹æ¡ˆ | K=5, stream(512) | å†…å­˜èŠ‚çœ 21%ï¼Œååé‡æŸå¤± < 5% |

### è®ºæ–‡è¡¨æ ¼æ¨¡æ¿

**Table 1: Performance Comparison**

```latex
\begin{table}[h]
\centering
\caption{Performance Comparison on Pythia-2.8B (Target) + Pythia-70M (Draft)}
\begin{tabular}{lcccccc}
\toprule
Method & Tokens & Throughput & TTFT & TPOT & Memory \\
       &        & (t/s)      & (ms) & (ms) & (MB)   \\
\midrule
Baseline         & 2000 & ~80        & ~40  & ~12  & ~3000 \\
Spec (K=5)       & 2000 & 192.9Â±3.8  & 37.2 & 5.17 & 2237  \\
Spec+Stream(256) & 2000 & 177.1Â±11.5 & 36.8 & 5.65 & 1688  \\
Spec+Stream(512) & 2000 & 187.7Â±4.9  & 37.2 & 5.32 & 1767  \\
\bottomrule
\end{tabular}
\end{table}
```

---

## ğŸ”§ ç¯å¢ƒè¦æ±‚

```bash
pip install torch transformers accelerate matplotlib numpy tqdm
```

### ç¡¬ä»¶é…ç½®
- GPU: NVIDIA GPU with CUDA
- æ˜¾å­˜: >= 8GBï¼ˆè¿è¡Œ Pythia-2.8Bï¼‰

### æ¨¡å‹è·¯å¾„
- Target Model: `/mnt/disk1/models/pythia-2.8b`
- Draft Model: `/mnt/disk1/models/pythia-70m`

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **PPL è§£é‡Š**ï¼šgreedy decoding ä¸‹ PPL æ¥è¿‘ 1 æ˜¯æ­£å¸¸çš„

2. **Acceptance Rate**ï¼šAccept% ä¸Šé™ä¸º 100%ï¼ŒT/Round > K è¡¨ç¤ºæœ‰ bonus tokens

3. **ç»“æœå¤ç°**ï¼šç”±äº GPU çŠ¶æ€ï¼Œç»“æœå¯èƒ½æœ‰ Â±5% æ³¢åŠ¨

---

*Last Updated: December 2024*
