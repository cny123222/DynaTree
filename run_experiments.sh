#!/bin/bash
# ====================================================================
# Tree-based Speculative Decoding - Paper Experiments Runner
# ====================================================================
# è¿™ä¸ªè„šæœ¬è¿è¡Œæ‰€æœ‰è®ºæ–‡éœ€è¦çš„å®éªŒ
# æ‰€æœ‰å‚æ•°éƒ½åŸºäºå®é™…å®éªŒç»“æœ (papers/Tree_Speculative_Decoding_å®éªŒæŠ¥å‘Š.md)
# ====================================================================

set -e  # Exit on error

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${GREEN}======================================================================"
echo "           Tree-based Speculative Decoding"
echo "                 è®ºæ–‡å®éªŒè¿è¡Œè„šæœ¬"
echo "======================================================================${NC}"
echo ""
echo "è¿™ä¸ªè„šæœ¬ä¼šè¿è¡Œä»¥ä¸‹å®éªŒï¼š"
echo "  1. ä¸»è¦æ€§èƒ½å¯¹æ¯” (Baseline vs Linear vs Tree-based)"
echo "  2. å‚æ•°å½±å“åˆ†æ (æ·±åº¦Dã€åˆ†æ”¯Bã€é˜ˆå€¼Ï„)"
echo "  3. æ¶ˆèå®éªŒ (åŠ¨æ€å‰ªæçš„æ•ˆæœ)"
echo "  4. åºåˆ—é•¿åº¦æ‰©å±• (100/200/300/500/1000 tokens)"
echo ""

# ====================================================================
# Step 1: æ£€æµ‹æ¨¡å‹è·¯å¾„
# ====================================================================
echo -e "${BLUE}[æ­¥éª¤ 1/5] æ£€æµ‹æ¨¡å‹è·¯å¾„...${NC}"
python3 check_models.py > /tmp/model_check.txt 2>&1

# Extract model paths from check output
if grep -q "/mnt/disk1/models/pythia-2.8b" /tmp/model_check.txt; then
    TARGET_MODEL="/mnt/disk1/models/pythia-2.8b"
elif [ -d "./models/pythia-2.8b" ]; then
    TARGET_MODEL="./models/pythia-2.8b"
else
    TARGET_MODEL="EleutherAI/pythia-2.8b"
fi

if grep -q "/mnt/disk1/models/pythia-70m" /tmp/model_check.txt; then
    DRAFT_MODEL="/mnt/disk1/models/pythia-70m"
elif [ -d "./models/pythia-70m" ]; then
    DRAFT_MODEL="./models/pythia-70m"
else
    DRAFT_MODEL="EleutherAI/pythia-70m"
fi

echo "  âœ“ Target Model: $TARGET_MODEL"
echo "  âœ“ Draft Model: $DRAFT_MODEL"
echo ""

# ====================================================================
# Step 2: åˆ›å»ºè¾“å‡ºç›®å½•
# ====================================================================
echo -e "${BLUE}[æ­¥éª¤ 2/5] åˆ›å»ºè¾“å‡ºç›®å½•...${NC}"
mkdir -p results/final_experiments
mkdir -p papers/figures/final

# Timestamp for this run
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
RESULTS_DIR="results/final_experiments/${TIMESTAMP}"
mkdir -p "$RESULTS_DIR"

echo "  âœ“ ç»“æœå°†ä¿å­˜åˆ°: $RESULTS_DIR"
echo ""

# ====================================================================
# Step 3: æ˜¾ç¤ºå®éªŒå‚æ•°è¯´æ˜
# ====================================================================
echo -e "${BLUE}[æ­¥éª¤ 3/5] å®éªŒå‚æ•°è¯´æ˜${NC}"
echo ""
echo -e "${YELLOW}æˆ‘ä»¬ä½¿ç”¨çš„å‚æ•°æ¥è‡ªå®é™…å®éªŒç»“æœ:${NC}"
echo "  å‚è€ƒæ–‡ä»¶: papers/Tree_Speculative_Decoding_å®éªŒæŠ¥å‘Š.md"
echo "  å‚æ•°æœç´¢æ–‡ä»¶: results/tree_param_search_20251231_140952.json"
echo ""
echo "æœ€ä¼˜å‚æ•° (Optimal Parameters - åŸºäºå®é™…æµ‹è¯•):"
echo "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
echo "  â”‚ Token Lengthâ”‚ Depth â”‚ Branch  â”‚ Thresholdâ”‚ Speedup (å®æµ‹)   â”‚"
echo "  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤"
echo "  â”‚  100 tokens â”‚   7   â”‚    3    â”‚   0.03   â”‚    1.43x         â”‚"
echo "  â”‚  200 tokens â”‚   7   â”‚    3    â”‚   0.03   â”‚    1.54x         â”‚"
echo "  â”‚  300 tokens â”‚   7   â”‚    3    â”‚   0.03   â”‚    1.60x         â”‚"
echo "  â”‚  500 tokens â”‚   8   â”‚    3    â”‚   0.03   â”‚ 1.62x-1.79x â­  â”‚"
echo "  â”‚ 1000 tokens â”‚   6   â”‚    3    â”‚   0.05   â”‚    1.71x         â”‚"
echo "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
echo ""
echo -e "${YELLOW}å…³é”®å‘ç°:${NC}"
echo "  â€¢ 500 tokens æ˜¯æœ€ä¼˜æµ‹è¯•é•¿åº¦ï¼ŒåŠ é€Ÿæ¯”æœ€é«˜"
echo "  â€¢ D=8, B=3, Ï„=0.03 æ˜¯ 500 tokens çš„æœ€ä¼˜é…ç½®"
echo "  â€¢ æ ‘æ·±åº¦éœ€è¦éšç”Ÿæˆé•¿åº¦è°ƒæ•´ (çŸ­åºåˆ—ç”¨D=7ï¼Œé•¿åºåˆ—ç”¨D=8)"
echo ""
echo -e "${YELLOW}å‚æ•°å«ä¹‰:${NC}"
echo "  â€¢ Depth (D): æ ‘çš„æœ€å¤§æ·±åº¦ï¼Œæ§åˆ¶æ¨æµ‹çš„æ­¥æ•°"
echo "  â€¢ Branch (B): æ¯ä¸ªèŠ‚ç‚¹çš„åˆ†æ”¯æ•°ï¼Œæ§åˆ¶æ¯æ­¥çš„å€™é€‰æ•°é‡"
echo "  â€¢ Threshold (Ï„): åŠ¨æ€å‰ªæé˜ˆå€¼ï¼Œæ¦‚ç‡ä½äºæ­¤å€¼çš„åˆ†æ”¯ä¼šè¢«å‰ªæ‰"
echo ""

# ====================================================================
# Experiment 1: ä¸»è¦æ€§èƒ½å¯¹æ¯” (500 tokens - æœ€ä¼˜é•¿åº¦)
# ====================================================================
echo ""
echo -e "${GREEN}======================================================================"
echo "å®éªŒ 1: ä¸»è¦æ€§èƒ½å¯¹æ¯” (Main Performance Comparison)"
echo -e "======================================================================${NC}"
echo ""
echo "è¿™ä¸ªå®éªŒå¯¹æ¯”æ‰€æœ‰æ–¹æ³•åœ¨æœ€ä¼˜é…ç½®ä¸‹çš„æ€§èƒ½"
echo ""
echo "æµ‹è¯•çš„æ–¹æ³•:"
echo "  1. Baseline: æ ‡å‡†è‡ªå›å½’ç”Ÿæˆ (æ— æ¨æµ‹è§£ç )"
echo "  2. HuggingFace Assisted: HF å®˜æ–¹å®ç°"
echo "  3. Linear Speculative Decoding: K=5,6,7,8"
echo "  4. Tree V2 (D=8, B=3, Ï„=0.03): æˆ‘ä»¬çš„æ–¹æ³• â­"
echo ""
echo "æµ‹è¯•é…ç½®:"
echo "  â€¢ ç”Ÿæˆé•¿åº¦: 500 tokens (æœ€ä¼˜é•¿åº¦)"
echo "  â€¢ æ ·æœ¬æ•°é‡: 5 prompts"
echo "  â€¢ æœ€ä¼˜å‚æ•°: D=8, B=3, Ï„=0.03"
echo ""
echo "è¯„ä¼°æŒ‡æ ‡:"
echo "  â€¢ Throughput (ååé‡): tokens/second"
echo "  â€¢ TPOT (æ¯tokenæ—¶é—´): ms/token"
echo "  â€¢ Acceptance Rate (æ¥å—ç‡): æ¨æµ‹tokenè¢«æ¥å—çš„æ¯”ä¾‹"
echo "  â€¢ Speedup (åŠ é€Ÿæ¯”): vs Baseline"
echo ""
echo "é¢„æœŸç»“æœ (åŸºäºå®é™…å®éªŒæŠ¥å‘Š):"
echo "  â€¢ Tree V2:       193.4 t/s (1.62x) â­ æœ€å¿«"
echo "  â€¢ HF Assisted:   161.9 t/s (1.36x)"
echo "  â€¢ Linear K=6:    133.1 t/s (1.11x)"
echo "  â€¢ Baseline:      119.4 t/s (1.00x)"
echo ""
echo "é¢„è®¡æ—¶é—´: ~15 åˆ†é’Ÿ (5æ¬¡è¿è¡Œï¼Œè·³è¿‡é¦–æ¬¡warmup)"
echo "å¯¹åº”è®ºæ–‡: Table 2 (ä¸»å®éªŒç»“æœè¡¨)"
echo ""

read -p "è¿è¡Œå®éªŒ 1? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo -e "${YELLOW}å¼€å§‹è¿è¡Œå®éªŒ 1...${NC}"
    python spec_decode/benchmark_tree_vs_linear.py \
        --target-model "$TARGET_MODEL" \
        --draft-model "$DRAFT_MODEL" \
        --max-new-tokens 500 \
        --num-prompts 5 \
        --device cuda \
        --save \
        --output-dir "$RESULTS_DIR"
    
    # Rename the output file
    LATEST_RESULT=$(ls -t "$RESULTS_DIR"/tree_vs_linear_benchmark_*.json 2>/dev/null | head -1)
    if [ -f "$LATEST_RESULT" ]; then
        mv "$LATEST_RESULT" "$RESULTS_DIR/exp1_main_comparison_500tokens.json"
        echo ""
        echo -e "${GREEN}âœ“ å®éªŒ 1 å®Œæˆ!${NC}"
        echo "  è¾“å‡ºæ–‡ä»¶: $RESULTS_DIR/exp1_main_comparison_500tokens.json"
    fi
else
    echo -e "${YELLOW}âŠ˜ è·³è¿‡å®éªŒ 1${NC}"
fi

# ====================================================================
# Experiment 2: å‚æ•°å½±å“åˆ†æ
# ====================================================================
echo ""
echo -e "${GREEN}======================================================================"
echo "å®éªŒ 2: å‚æ•°å½±å“åˆ†æ (Parameter Sweep Visualization)"
echo -e "======================================================================${NC}"
echo ""
echo "è¿™ä¸ªå®éªŒå¯è§†åŒ–è¶…å‚æ•°å¯¹æ€§èƒ½çš„å½±å“"
echo ""
echo "å‚æ•°æœç´¢èŒƒå›´ (å·²å®Œæˆçš„å®éªŒ):"
echo "  â€¢ æ·±åº¦ (Depth):     [3, 4, 5, 6, 7, 8]"
echo "  â€¢ åˆ†æ”¯å› å­ (Branch): [2, 3, 4]"
echo "  â€¢ å‰ªæé˜ˆå€¼ (Threshold): [0.01, 0.02, 0.03, 0.05, 0.1]"
echo "  â€¢ Tokené•¿åº¦:        [100, 200, 300, 500, 1000]"
echo "  â€¢ æ€»é…ç½®æ•°: 6 Ã— 3 Ã— 5 Ã— 5 = 450 ç»„"
echo ""
echo "å…³é”®å‘ç°:"
echo "  â€¢ B=3 æ˜¯æœ€ä¼˜åˆ†æ”¯å› å­ (å¹³å‡åŠ é€Ÿ 1.31x)"
echo "  â€¢ Ï„=0.03 æ˜¯æœ€ä¼˜é˜ˆå€¼ (æœ€å¤§åŠ é€Ÿ 1.79x)"
echo "  â€¢ æ·±åº¦éœ€è¦æ ¹æ®tokené•¿åº¦è°ƒæ•´"
echo ""
echo "æ•°æ®æ¥æº:"
echo "  â€¢ æ–‡ä»¶: results/tree_param_search_20251231_140952.json"
echo "  â€¢ åŒ…å« 450 ç»„å‚æ•°é…ç½®çš„å®Œæ•´æµ‹è¯•ç»“æœ"
echo ""
echo "ç”Ÿæˆå›¾è¡¨:"
echo "  â€¢ çƒ­åŠ›å›¾: å±•ç¤ºä¸åŒå‚æ•°ç»„åˆçš„ååé‡"
echo "  â€¢ æ›²çº¿å›¾: å±•ç¤ºå•ä¸ªå‚æ•°çš„å½±å“è¶‹åŠ¿"
echo "  â€¢ Top-10 æœ€ä¼˜é…ç½®åˆ—è¡¨"
echo ""
echo "é¢„è®¡æ—¶é—´: ~5 åˆ†é’Ÿ"
echo "å¯¹åº”è®ºæ–‡: Figure 2 (å‚æ•°å½±å“å›¾)"
echo ""

read -p "è¿è¡Œå®éªŒ 2? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    # Check if we have existing param search data
    PARAM_SEARCH_FILE=$(ls -t results/tree_param_search_*.json 2>/dev/null | head -1)
    
    if [ -f "$PARAM_SEARCH_FILE" ]; then
        echo ""
        echo -e "${YELLOW}ä½¿ç”¨å·²æœ‰å‚æ•°æœç´¢æ•°æ®: $PARAM_SEARCH_FILE${NC}"
        
        # Check if plotting script exists
        if [ -f "papers/plot_param_sweep_publication.py" ]; then
            python papers/plot_param_sweep_publication.py \
                --input "$PARAM_SEARCH_FILE" \
                --output "$RESULTS_DIR/exp2_param_sweep.pdf" \
                --style publication
            
            echo ""
            echo -e "${GREEN}âœ“ å®éªŒ 2 å®Œæˆ!${NC}"
            echo "  è¾“å‡ºæ–‡ä»¶: $RESULTS_DIR/exp2_param_sweep.pdf"
        else
            echo -e "${YELLOW}è­¦å‘Š: papers/plot_param_sweep_publication.py ä¸å­˜åœ¨${NC}"
            echo "ä½¿ç”¨åˆ†æè„šæœ¬ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š..."
            
            # Use the analysis script as fallback
            python papers/analyze_tree_search_results.py "$PARAM_SEARCH_FILE" \
                > "$RESULTS_DIR/exp2_param_analysis.txt"
            
            echo ""
            echo -e "${GREEN}âœ“ å®éªŒ 2 å®Œæˆ (æ–‡æœ¬åˆ†æ)${NC}"
            echo "  è¾“å‡ºæ–‡ä»¶: $RESULTS_DIR/exp2_param_analysis.txt"
        fi
    else
        echo ""
        echo -e "${RED}âœ— é”™è¯¯: æœªæ‰¾åˆ°å‚æ•°æœç´¢æ•°æ®!${NC}"
        echo "è¯·å…ˆè¿è¡Œå‚æ•°æœç´¢:"
        echo "  python papers/tree_param_search.py"
    fi
else
    echo -e "${YELLOW}âŠ˜ è·³è¿‡å®éªŒ 2${NC}"
fi

# ====================================================================
# Experiment 3: æ¶ˆèå®éªŒ (Pruning Ablation)
# ====================================================================
echo ""
echo -e "${GREEN}======================================================================"
echo "å®éªŒ 3: æ¶ˆèå®éªŒ - åŠ¨æ€å‰ªææ•ˆæœ (Ablation Study)"
echo -e "======================================================================${NC}"
echo ""
echo "è¿™ä¸ªå®éªŒæµ‹è¯•åŠ¨æ€å‰ªæ(Dynamic Pruning)çš„æœ‰æ•ˆæ€§"
echo ""
echo "æµ‹è¯•çš„ä¸‰ä¸ªå˜ä½“:"
echo "  1. No Pruning (æ— å‰ªæ)"
echo "     â€¢ threshold=0.0, max_nodes=9999"
echo "     â€¢ æ ‘ä¼šéå¸¸å¤§ï¼ŒåŒ…å«æ‰€æœ‰å¯èƒ½çš„åˆ†æ”¯"
echo "     â€¢ é¢„æœŸ: é€Ÿåº¦æ…¢ï¼Œæ˜¾å­˜å ç”¨é«˜"
echo ""
echo "  2. Static Pruning (é™æ€å‰ªæ)"
echo "     â€¢ threshold=0.0, max_nodes=64"
echo "     â€¢ é€šè¿‡å›ºå®šèŠ‚ç‚¹æ•°é™åˆ¶æ ‘çš„å¤§å°"
echo "     â€¢ é¢„æœŸ: ä¸­ç­‰æ€§èƒ½"
echo ""
echo "  3. Dynamic Pruning (åŠ¨æ€å‰ªæ) â† æˆ‘ä»¬çš„æ–¹æ³•"
echo "     â€¢ threshold=0.03, max_nodes=128"
echo "     â€¢ æ ¹æ®æ¦‚ç‡åŠ¨æ€å‰ªæ‰ä¸å¤ªå¯èƒ½çš„åˆ†æ”¯"
echo "     â€¢ é¢„æœŸ: æœ€ä½³æ€§èƒ½ (1.62xåŠ é€Ÿ)"
echo ""
echo "è¯„ä¼°æŒ‡æ ‡:"
echo "  â€¢ Throughput: ååé‡"
echo "  â€¢ Avg Nodes: å¹³å‡æ ‘èŠ‚ç‚¹æ•°"
echo "  â€¢ Avg Path Length: å¹³å‡æ¥å—è·¯å¾„é•¿åº¦"
echo "  â€¢ Acceptance Rate: æ¥å—ç‡"
echo ""
echo "é¢„è®¡æ—¶é—´: ~20 åˆ†é’Ÿ"
echo "å¯¹åº”è®ºæ–‡: Table 3 (æ¶ˆèå®éªŒè¡¨)"
echo ""

read -p "è¿è¡Œå®éªŒ 3? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo ""
    echo -e "${YELLOW}å¼€å§‹è¿è¡Œå®éªŒ 3...${NC}"
    
    # Create ablation script if it doesn't exist
    if [ ! -f "spec_decode/ablation_pruning.py" ]; then
        echo "åˆ›å»ºæ¶ˆèå®éªŒè„šæœ¬..."
        cat > spec_decode/ablation_pruning.py << 'ABLATION_SCRIPT'
#!/usr/bin/env python3
"""
Ablation Study: Dynamic Pruning in Tree-based Speculative Decoding

This script compares three pruning strategies:
1. No Pruning: Allow tree to grow without limits
2. Static Pruning: Fixed maximum node count
3. Dynamic Pruning: Probability-based pruning (our method)
"""
import argparse
import json
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
import sys
import os

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from spec_decode.core import TreeSpeculativeGeneratorV2
import time

def run_ablation(args):
    print("=" * 70)
    print("Ablation Study: Dynamic Pruning Effect")
    print("=" * 70)
    print("\nLoading models...")
    
    target_model = AutoModelForCausalLM.from_pretrained(
        args.target_model,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    target_model.eval()
    
    draft_model = AutoModelForCausalLM.from_pretrained(
        args.draft_model,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    draft_model.eval()
    
    tokenizer = AutoTokenizer.from_pretrained(args.target_model)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Test prompts - same as in the report
    prompts = [
        "Write a detailed technical explanation about the development of large language models. Cover the history, architecture innovations, training techniques, and future directions.",
        "Explain the concept of neural networks and deep learning, including their applications in computer vision and natural language processing.",
        "Discuss the challenges and opportunities in artificial intelligence research, focusing on ethical considerations and societal impact.",
        "Describe the evolution of programming languages from assembly to modern high-level languages, highlighting key innovations.",
        "Analyze the impact of quantum computing on cryptography and information security in the coming decades.",
    ]
    
    # Three pruning strategies - matching the actual experiment configuration
    variants = {
        "no_prune": {
            "name": "No Pruning",
            "threshold": 0.0,
            "max_nodes": 9999,
            "description": "Allow unlimited tree growth"
        },
        "static_prune": {
            "name": "Static Pruning",
            "threshold": 0.0,
            "max_nodes": 64,
            "description": "Fixed max nodes limit"
        },
        "dynamic_prune": {
            "name": "Dynamic Pruning (Ours)",
            "threshold": 0.03,  # Optimal from actual experiments
            "max_nodes": 128,   # Optimal from actual experiments
            "description": "Probability-based pruning"
        },
    }
    
    results = {}
    
    for variant_id, variant_config in variants.items():
        print(f"\n{'=' * 70}")
        print(f"Testing: {variant_config['name']}")
        print(f"  Description: {variant_config['description']}")
        print(f"  Threshold: {variant_config['threshold']}")
        print(f"  Max Nodes: {variant_config['max_nodes']}")
        print("=" * 70)
        
        generator = TreeSpeculativeGeneratorV2(
            target_model=target_model,
            draft_model=draft_model,
            tokenizer=tokenizer,
            tree_depth=args.depth,
            branch_factor=args.branch,
            probability_threshold=variant_config["threshold"],
            max_tree_nodes=variant_config["max_nodes"],
            device="cuda",
            use_compile=False
        )
        
        times = []
        stats_list = []
        
        # Run multiple times, skip first warmup
        for run_idx in range(args.num_runs + 1):
            for prompt in tqdm(prompts, desc=f"Run {run_idx+1}/{args.num_runs+1}"):
                generator.reset()
                
                torch.cuda.synchronize()
                start = time.perf_counter()
                
                output = generator.generate(prompt, max_new_tokens=args.max_new_tokens)
                
                torch.cuda.synchronize()
                elapsed = time.perf_counter() - start
                
                stats = generator.get_stats()
                
                # Skip first run (warmup)
                if run_idx > 0:
                    times.append(elapsed)
                    stats_list.append(stats)
        
        # Calculate average results
        avg_time = sum(times) / len(times)
        avg_tokens = sum(s['total_tokens'] for s in stats_list) / len(stats_list)
        avg_rounds = sum(s['total_rounds'] for s in stats_list) / len(stats_list)
        avg_nodes = sum(s['total_tree_nodes'] for s in stats_list) / avg_rounds if avg_rounds > 0 else 0
        avg_path = sum(s['avg_accepted_path_length'] for s in stats_list) / len(stats_list)
        avg_acceptance = sum(s.get('acceptance_rate', 0) for s in stats_list) / len(stats_list)
        throughput = avg_tokens / avg_time if avg_time > 0 else 0
        tpot_ms = (avg_time / avg_tokens) * 1000 if avg_tokens > 0 else 0
        
        results[variant_id] = {
            "name": variant_config["name"],
            "config": {
                "threshold": variant_config["threshold"],
                "max_nodes": variant_config["max_nodes"]
            },
            "metrics": {
                "throughput": round(throughput, 2),
                "tpot_ms": round(tpot_ms, 2),
                "avg_nodes_per_round": round(avg_nodes, 2),
                "avg_path_length": round(avg_path, 2),
                "acceptance_rate": round(avg_acceptance * 100, 1),
                "avg_time": round(avg_time, 3),
                "avg_tokens": round(avg_tokens, 1)
            }
        }
        
        print(f"\n{variant_config['name']} Results:")
        print(f"  Throughput: {throughput:.1f} tokens/s")
        print(f"  TPOT: {tpot_ms:.2f} ms")
        print(f"  Avg Nodes/Round: {avg_nodes:.1f}")
        print(f"  Avg Path Length: {avg_path:.2f}")
        print(f"  Acceptance Rate: {avg_acceptance*100:.1f}%")
    
    # Save results
    output_file = args.output
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    # Print summary comparison
    print(f"\n{'=' * 70}")
    print("SUMMARY COMPARISON")
    print("=" * 70)
    print(f"{'Method':<30} {'Throughput':>12} {'TPOT':>10} {'Nodes':>8} {'Path':>8}")
    print("-" * 70)
    
    baseline_throughput = results.get("dynamic_prune", {}).get("metrics", {}).get("throughput", 1)
    
    for variant_id in ["no_prune", "static_prune", "dynamic_prune"]:
        res = results[variant_id]
        metrics = res['metrics']
        speedup = metrics['throughput'] / baseline_throughput if baseline_throughput > 0 else 1.0
        print(f"{res['name']:<30} {metrics['throughput']:>10.1f} t/s "
              f"{metrics['tpot_ms']:>8.2f} ms "
              f"{metrics['avg_nodes_per_round']:>6.1f} "
              f"{metrics['avg_path_length']:>6.2f}")
    
    print(f"\nResults saved to: {output_file}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--target-model", required=True, help="Target model path")
    parser.add_argument("--draft-model", required=True, help="Draft model path")
    parser.add_argument("--depth", type=int, default=8, help="Tree depth (default: 8)")
    parser.add_argument("--branch", type=int, default=3, help="Branch factor (default: 3)")
    parser.add_argument("--max-new-tokens", type=int, default=500, help="Tokens to generate (default: 500)")
    parser.add_argument("--num-runs", type=int, default=4, help="Number of runs (default: 4, +1 warmup)")
    parser.add_argument("--output", default="results/ablation_pruning.json", help="Output file")
    args = parser.parse_args()
    
    run_ablation(args)
ABLATION_SCRIPT
        chmod +x spec_decode/ablation_pruning.py
        echo "âœ“ æ¶ˆèå®éªŒè„šæœ¬åˆ›å»ºå®Œæˆ"
    fi
    
    python spec_decode/ablation_pruning.py \
        --target-model "$TARGET_MODEL" \
        --draft-model "$DRAFT_MODEL" \
        --depth 8 \
        --branch 3 \
        --max-new-tokens 500 \
        --num-runs 4 \
        --output "$RESULTS_DIR/exp3_ablation_pruning.json"
    
    echo ""
    echo -e "${GREEN}âœ“ å®éªŒ 3 å®Œæˆ!${NC}"
    echo "  è¾“å‡ºæ–‡ä»¶: $RESULTS_DIR/exp3_ablation_pruning.json"
else
    echo -e "${YELLOW}âŠ˜ è·³è¿‡å®éªŒ 3${NC}"
fi

# ====================================================================
# Experiment 4: åºåˆ—é•¿åº¦æ‰©å±• (Scalability)
# ====================================================================
echo ""
echo -e "${GREEN}======================================================================"
echo "å®éªŒ 4: åºåˆ—é•¿åº¦æ‰©å±•æµ‹è¯• (Sequence Length Scaling)"
echo -e "======================================================================${NC}"
echo ""
echo "è¿™ä¸ªå®éªŒæµ‹è¯•ä¸åŒç”Ÿæˆé•¿åº¦ä¸‹çš„æ€§èƒ½ï¼ŒéªŒè¯æ–¹æ³•çš„å¯æ‰©å±•æ€§"
echo ""
echo "æµ‹è¯•é…ç½® (ä½¿ç”¨å„è‡ªçš„æœ€ä¼˜å‚æ•°):"
echo "  â€¢ 100 tokens:  D=7, B=3, Ï„=0.03 (é¢„æœŸ 1.43x)"
echo "  â€¢ 200 tokens:  D=7, B=3, Ï„=0.03 (é¢„æœŸ 1.54x)"
echo "  â€¢ 300 tokens:  D=7, B=3, Ï„=0.03 (é¢„æœŸ 1.60x)"
echo "  â€¢ 500 tokens:  D=8, B=3, Ï„=0.03 (é¢„æœŸ 1.62x) â­ æœ€ä¼˜"
echo "  â€¢ 1000 tokens: D=6, B=3, Ï„=0.05 (é¢„æœŸ 1.71x)"
echo ""
echo "å…³é”®å‘ç°:"
echo "  â€¢ ç”Ÿæˆé•¿åº¦è¶Šé•¿ï¼ŒTree-basedæ–¹æ³•çš„ä¼˜åŠ¿ä¸€èˆ¬è¶Šæ˜æ˜¾"
echo "  â€¢ 500 tokens è¾¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹"
echo "  â€¢ ä¸åŒé•¿åº¦éœ€è¦ä¸åŒçš„æ ‘æ·±åº¦"
echo ""
echo "é¢„è®¡æ—¶é—´: ~60 åˆ†é’Ÿ (äº”ä¸ªé•¿åº¦åˆ†åˆ«æµ‹è¯•)"
echo "å¯¹åº”è®ºæ–‡: Figure 3 æˆ– Table 4 (æ‰©å±•æ€§åˆ†æ)"
echo ""

read -p "è¿è¡Œå®éªŒ 4? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo ""
    echo -e "${YELLOW}å¼€å§‹è¿è¡Œå®éªŒ 4...${NC}"
    echo ""
    
    # Create length scaling script with optimal params for each length
    cat > /tmp/run_length_scaling.py << 'LENGTH_SCALING_SCRIPT'
#!/usr/bin/env python3
"""
Sequence Length Scaling Experiment
Tests performance across different generation lengths with optimal parameters.
Based on actual experimental results from papers/Tree_Speculative_Decoding_å®éªŒæŠ¥å‘Š.md
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import json
import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer
from spec_decode.core import TreeSpeculativeGeneratorV2

# Optimal parameters from actual experiments for each length
configs = [
    {"length": 100, "depth": 7, "branch": 3, "threshold": 0.03},
    {"length": 200, "depth": 7, "branch": 3, "threshold": 0.03},
    {"length": 300, "depth": 7, "branch": 3, "threshold": 0.03},
    {"length": 500, "depth": 8, "branch": 3, "threshold": 0.03},
    {"length": 1000, "depth": 6, "branch": 3, "threshold": 0.05},
]

target_model_path = sys.argv[1]
draft_model_path = sys.argv[2]
output_dir = sys.argv[3]

print("=" * 70)
print("Sequence Length Scaling Experiment")
print("=" * 70)
print("\nLoading models...")

tokenizer = AutoTokenizer.from_pretrained(target_model_path)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

target_model = AutoModelForCausalLM.from_pretrained(
    target_model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)
target_model.eval()

draft_model = AutoModelForCausalLM.from_pretrained(
    draft_model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)
draft_model.eval()

test_prompt = "Write a detailed technical explanation about the development of large language models."

all_length_results = {}

for config in configs:
    length = config["length"]
    depth = config["depth"]
    branch = config["branch"]
    threshold = config["threshold"]
    
    print(f"\n{'=' * 70}")
    print(f"Testing {length} tokens")
    print(f"  Optimal params: D={depth}, B={branch}, Ï„={threshold}")
    print("=" * 70)
    
    # Test Baseline
    print("\n[1/2] Testing Baseline...")
    baseline_times = []
    for run in range(5):
        torch.cuda.empty_cache()
        input_ids = tokenizer(test_prompt, return_tensors="pt").input_ids.to("cuda")
        
        torch.cuda.synchronize()
        start = time.perf_counter()
        
        with torch.inference_mode():
            outputs = target_model.generate(
                input_ids,
                max_new_tokens=length,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id
            )
        
        torch.cuda.synchronize()
        elapsed = time.perf_counter() - start
        
        if run > 0:  # Skip warmup
            baseline_times.append(elapsed)
    
    baseline_throughput = length / (sum(baseline_times) / len(baseline_times))
    print(f"  Baseline: {baseline_throughput:.1f} t/s")
    
    # Test Tree V2 with optimal params
    print("\n[2/2] Testing Tree V2...")
    generator = TreeSpeculativeGeneratorV2(
        target_model=target_model,
        draft_model=draft_model,
        tokenizer=tokenizer,
        tree_depth=depth,
        branch_factor=branch,
        probability_threshold=threshold,
        max_tree_nodes=128,
        device="cuda",
        use_compile=False
    )
    
    tree_times = []
    stats_list = []
    
    for run in range(5):
        generator.reset()
        torch.cuda.empty_cache()
        
        torch.cuda.synchronize()
        start = time.perf_counter()
        
        output = generator.generate(test_prompt, max_new_tokens=length)
        
        torch.cuda.synchronize()
        elapsed = time.perf_counter() - start
        
        if run > 0:  # Skip warmup
            tree_times.append(elapsed)
            stats_list.append(generator.get_stats())
    
    tree_throughput = length / (sum(tree_times) / len(tree_times))
    speedup = tree_throughput / baseline_throughput
    avg_acceptance = sum(s.get('acceptance_rate', 0) for s in stats_list) / len(stats_list)
    
    print(f"  Tree V2: {tree_throughput:.1f} t/s ({speedup:.2f}x speedup)")
    
    all_length_results[f"{length}_tokens"] = {
        "config": {
            "length": length,
            "depth": depth,
            "branch": branch,
            "threshold": threshold
        },
        "results": {
            "baseline_throughput": round(baseline_throughput, 2),
            "tree_throughput": round(tree_throughput, 2),
            "speedup": round(speedup, 2),
            "acceptance_rate": round(avg_acceptance * 100, 1)
        }
    }

# Save combined results
output_file = f"{output_dir}/exp4_length_scaling.json"
with open(output_file, 'w') as f:
    json.dump(all_length_results, f, indent=2)

# Print summary
print(f"\n{'=' * 70}")
print("SUMMARY")
print("=" * 70)
print(f"{'Length':<10} {'Baseline':>12} {'Tree V2':>12} {'Speedup':>10}")
print("-" * 70)

for length in [100, 200, 300, 500, 1000]:
    key = f"{length}_tokens"
    if key in all_length_results:
        res = all_length_results[key]["results"]
        print(f"{length:>5} tok {res['baseline_throughput']:>10.1f} t/s "
              f"{res['tree_throughput']:>10.1f} t/s {res['speedup']:>8.2f}x")

print(f"\n{'=' * 70}")
print(f"Results saved to: {output_file}")
print("=" * 70)
LENGTH_SCALING_SCRIPT
    
    python /tmp/run_length_scaling.py \
        "$TARGET_MODEL" \
        "$DRAFT_MODEL" \
        "$RESULTS_DIR"
    
    echo ""
    echo -e "${GREEN}âœ“ å®éªŒ 4 å®Œæˆ!${NC}"
    echo "  è¾“å‡ºæ–‡ä»¶: $RESULTS_DIR/exp4_length_scaling.json"
else
    echo -e "${YELLOW}âŠ˜ è·³è¿‡å®éªŒ 4${NC}"
fi

# ====================================================================
# Summary
# ====================================================================
echo ""
echo -e "${GREEN}======================================================================"
echo "                    å®éªŒå®Œæˆæ€»ç»“"
echo "======================================================================${NC}"
echo ""
echo "æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: $RESULTS_DIR"
echo ""

# List generated files
echo "ç”Ÿæˆçš„æ–‡ä»¶:"
for file in "$RESULTS_DIR"/*; do
    if [ -f "$file" ]; then
        filename=$(basename "$file")
        filesize=$(du -h "$file" | cut -f1)
        echo "  âœ“ $filename ($filesize)"
    fi
done

echo ""
echo -e "${YELLOW}ä¸‹ä¸€æ­¥æ“ä½œ:${NC}"
echo ""
echo "1. æŸ¥çœ‹å®éªŒç»“æœ:"
echo "   cd $RESULTS_DIR"
echo "   cat exp1_main_comparison_500tokens.json | jq '.results[] | {method, avg_throughput, tpot_ms}'"
echo ""
echo "2. ç”Ÿæˆè®ºæ–‡å›¾è¡¨:"
echo "   python papers/generate_all_figures.py --results-dir $RESULTS_DIR"
echo ""
echo "3. å¼€å§‹å†™è®ºæ–‡:"
echo "   cd papers/"
echo "   # æŸ¥çœ‹ PAPER_PLAN.md è·å–è¯¦ç»†çš„å†™ä½œæŒ‡å—"
echo ""
echo -e "${GREEN}å®éªŒæ€»ç»“ (åŸºäºå®é™…å®éªŒæŠ¥å‘Š):${NC}"
echo "  â€¢ å®éªŒ 1: ä¸»è¦æ€§èƒ½å¯¹æ¯” â†’ è®ºæ–‡ Table 2"
echo "  â€¢ å®éªŒ 2: å‚æ•°å½±å“åˆ†æ â†’ è®ºæ–‡ Figure 2"
echo "  â€¢ å®éªŒ 3: æ¶ˆèå®éªŒ     â†’ è®ºæ–‡ Table 3"
echo "  â€¢ å®éªŒ 4: é•¿åº¦æ‰©å±•æ€§   â†’ è®ºæ–‡ Figure 3/Table 4"
echo ""
echo -e "${BLUE}è®ºæ–‡å…³é”®æ•°æ® (å®é™…æµ‹è¯•ç»“æœ):${NC}"
echo "  ğŸ† Tree V2 (D=8, B=3, Ï„=0.03):  193.4 t/s (1.62x)"
echo "  ğŸ¥ˆ HuggingFace Assisted:         161.9 t/s (1.36x)"
echo "  ğŸ¥‰ Linear K=6:                   133.1 t/s (1.11x)"
echo "  ğŸ“Š Baseline:                     119.4 t/s (1.00x)"
echo ""
echo "  å‚æ•°æœç´¢æœ€ä½³ç»“æœ: 1.79x (500 tokens, D=8, B=3, Ï„=0.03)"
echo ""
echo -e "${GREEN}========================================== å®Œæˆ ==========================================${NC}"
echo ""
