# Tree-based Speculative Decoding è®ºæ–‡è§„åˆ’

**ç›®æ ‡æœŸåˆŠ**: NeurIPS 2025  
**æˆªæ­¢æ—¥æœŸ**: 2026å¹´1æœˆ5æ—¥  
**é¡µæ•°é™åˆ¶**: 4é¡µæ­£æ–‡ï¼ˆä¸å«referencesï¼‰

---

## ğŸ“ è®ºæ–‡æ ‡é¢˜ï¼ˆå€™é€‰ï¼‰

### é¦–é€‰ï¼š
**"Tree-based Speculative Decoding with Dynamic Pruning for Efficient LLM Inference"**

### å¤‡é€‰ï¼š
- "Beyond Linear Speculation: Tree-based Token Generation for Accelerated LLM Inference"
- "Parallel Path Exploration for Speculative Decoding in Large Language Models"

---

## ğŸ¯ æ ¸å¿ƒè´¡çŒ®

### ä¸»è¦è´¡çŒ®ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰ï¼š

1. **Tree-based Drafting with Dynamic Pruning** â­â­â­â­â­
   - æå‡ºå¤šåˆ†æ”¯tokenæ ‘ç”Ÿæˆç­–ç•¥
   - åŠ¨æ€å‰ªææ§åˆ¶æ ‘å¤§å°
   - ç›¸æ¯”linearæ–¹æ³•æå‡25%

2. **Tree Attention Mechanism** â­â­â­â­
   - 4D attention maskæ”¯æŒå¹¶è¡ŒéªŒè¯
   - ä¸€æ¬¡forwardéªŒè¯æ•´æ£µæ ‘
   - ä¿æŒcorrectnessï¼ˆgreedy decodingä¸€è‡´ï¼‰

3. **Systematic Hyperparameter Analysis** â­â­â­
   - Depth, Branch Factor, Thresholdçš„trade-off
   - ä¸åŒç”Ÿæˆé•¿åº¦çš„æœ€ä¼˜é…ç½®
   - å®ç”¨éƒ¨ç½²æŒ‡å—

4. **Open-source Implementation** â­â­â­
   - å®Œæ•´å¯å¤ç°ä»£ç 
   - è¯¦ç»†benchmarkå·¥å…·
   - ä¸HuggingFaceç”Ÿæ€å…¼å®¹

---

## ğŸ“– æ•…äº‹çº¿ï¼ˆNarrative Arcï¼‰

### æ•´ä½“æ•…äº‹ç»“æ„ï¼š

```
ã€é—®é¢˜ã€‘â†’ã€ç°æœ‰æ–¹æ¡ˆã€‘â†’ã€å±€é™ã€‘â†’ã€æˆ‘ä»¬çš„åˆ›æ–°ã€‘â†’ã€æŠ€æœ¯æŒ‘æˆ˜ã€‘â†’ã€è§£å†³æ–¹æ¡ˆã€‘â†’ã€æ•ˆæœéªŒè¯ã€‘â†’ã€æ·±å…¥åˆ†æã€‘
```

### è¯¦ç»†æ•…äº‹å±•å¼€ï¼š

#### 1. é—®é¢˜èƒŒæ™¯ï¼ˆWhy care?ï¼‰
**ç—›ç‚¹**: 
- LLMæ¨ç†æ…¢ï¼ˆautoregressiveç“¶é¢ˆï¼‰
- æ¯æ¬¡ç”Ÿæˆ1ä¸ªtokenï¼ŒGPUåˆ©ç”¨ç‡ä½
- å®é™…åº”ç”¨éœ€æ±‚ï¼šå®æ—¶å¯¹è¯ã€é•¿æ–‡æœ¬ç”Ÿæˆ

**é‡åŒ–æ•°æ®**:
- Baseline: 60.8 tokens/s
- ç”Ÿæˆ100ä¸ªtokenéœ€è¦ ~1.6ç§’
- å¯¹è¯å»¶è¿Ÿæ˜æ˜¾

#### 2. ç°æœ‰æ–¹æ¡ˆï¼ˆWhat exists?ï¼‰
**Linear Speculative Decoding**:
```
Draft ModelçŒœæµ‹: [t1, t2, t3, t4, t5]
Target ModeléªŒè¯: âœ“   âœ“   âœ—   âœ—   âœ—
æ¥å—: [t1, t2] + bonus â†’ 3 tokens/round
```

**ä¼˜ç‚¹**: 
- ç®€å•ï¼Œæ˜“å®ç°
- å½“draftå‡†ç¡®æ—¶æ•ˆæœå¥½

**å±€é™**:
- åªæœ‰ä¸€æ¡è·¯å¾„
- å¦‚æœt2é”™äº†ï¼Œt3-t5å…¨éƒ¨æµªè´¹
- æ¥å—ç‡â‰ˆ70-80%ï¼Œæœ‰æå‡ç©ºé—´

#### 3. æˆ‘ä»¬çš„æ´å¯Ÿï¼ˆKey Insightï¼‰
**å…³é”®é—®é¢˜**: ä¸ºä»€ä¹ˆåªçŒœä¸€æ¡è·¯å¾„ï¼Ÿ

**æ ¸å¿ƒæ´å¯Ÿ**:
```
Linear: çŒœ5ä¸ªtokenï¼Œæ¥å—ç‡80% â†’ æœŸæœ›4ä¸ªtoken
Tree:   çŒœ5ä¸ªä½ç½®Ã—3ä¸ªåˆ†æ”¯ï¼Œè‡³å°‘ä¸€æ¡å¯¹çš„æ¦‚ç‡æ›´é«˜ï¼
```

**ç±»æ¯”**: 
- Linear = å•çº¿ç¨‹æœç´¢
- Tree = å¤šçº¿ç¨‹å¹¶è¡Œæœç´¢
- åªè¦æœ‰ä¸€æ¡è·¯å¯¹ï¼Œå°±èƒ½è¢«æ¥å—

#### 4. æŠ€æœ¯æŒ‘æˆ˜ï¼ˆChallengesï¼‰

**Challenge 1: æ ‘å¤ªå¤§**
```
æ·±åº¦D=5, åˆ†æ”¯B=3
ç†è®ºèŠ‚ç‚¹æ•° = 1 + 3 + 9 + 27 + 81 + 243 = 364ä¸ªtoken
â†’ å¤ªå¤šäº†ï¼draft modelä¼šå¾ˆæ…¢
```

**Challenge 2: å¦‚ä½•å¹¶è¡ŒéªŒè¯**
```
æ ‘ä¸æ˜¯çº¿æ€§åºåˆ—ï¼Œå¦‚ä½•è®©target modelä¸€æ¬¡å¤„ç†ï¼Ÿ
â†’ éœ€è¦ç‰¹æ®Šçš„attention mask
```

**Challenge 3: é€‰æ‹©å“ªæ¡è·¯å¾„**
```
éªŒè¯åå¯èƒ½æœ‰å¤šæ¡è·¯å¾„éƒ¨åˆ†æ­£ç¡®
â†’ éœ€è¦é€‰æ‹©ç­–ç•¥ï¼ˆè´ªå¿ƒï¼Ÿæœ€é•¿ï¼Ÿï¼‰
```

#### 5. æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆï¼ˆOur Solutionï¼‰

**Solution 1: åŠ¨æ€å‰ªæï¼ˆDynamic Pruningï¼‰**
```python
# ç”Ÿæˆæ¯ä¸ªåˆ†æ”¯æ—¶æ£€æŸ¥æ¦‚ç‡
if p(token|prefix) < threshold:
    prune this branch  # ä¸ç»§ç»­æ‰©å±•
    
# åŒæ—¶é™åˆ¶æ€»èŠ‚ç‚¹æ•°
if num_nodes >= max_nodes:
    stop expansion
```

**æ•ˆæœ**:
- å¹³å‡èŠ‚ç‚¹æ•°: 364 â†’ 22.7
- ä¿æŒé«˜è´¨é‡è·¯å¾„
- å‰ªæâ‰ éšæœºåˆ é™¤ï¼Œæ˜¯åŸºäºæ¦‚ç‡çš„æ™ºèƒ½å‰ªæ

**Solution 2: Tree Attention Mask**
```
æ ‘ç»“æ„:
    0 (root)
    â”œâ”€ 1 (child1)
    â”‚  â”œâ”€ 3 (grandchild)
    â”‚  â””â”€ 4
    â””â”€ 2 (child2)
       â””â”€ 5

æ‰å¹³åŒ–åºåˆ—: [0, 1, 2, 3, 4, 5]

Attention Mask (6x6):
     0  1  2  3  4  5
  0  âœ“  âœ—  âœ—  âœ—  âœ—  âœ—   # rootä¸çœ‹åä»£
  1  âœ“  âœ“  âœ—  âœ—  âœ—  âœ—   # 1çœ‹rootå’Œè‡ªå·±
  2  âœ“  âœ—  âœ“  âœ—  âœ—  âœ—   # 2çœ‹rootå’Œè‡ªå·±
  3  âœ“  âœ“  âœ—  âœ“  âœ—  âœ—   # 3çœ‹root,1å’Œè‡ªå·±
  4  âœ“  âœ“  âœ—  âœ—  âœ“  âœ—   # 4çœ‹root,1å’Œè‡ªå·±
  5  âœ“  âœ—  âœ“  âœ—  âœ—  âœ“   # 5çœ‹root,2å’Œè‡ªå·±
```

**å…³é”®**: æ¯ä¸ªèŠ‚ç‚¹åªçœ‹åˆ°å®ƒçš„ç¥–å…ˆè·¯å¾„ï¼

**Solution 3: è´ªå¿ƒæœ€é•¿åŒ¹é…è·¯å¾„**
```python
def find_best_path(tree, target_logits):
    """é€‰æ‹©æœ€é•¿çš„åŒ¹é…è·¯å¾„"""
    paths = tree.get_all_leaf_paths()
    
    for path in sorted(paths, key=len, reverse=True):
        matched_length = 0
        for node in path:
            target_pred = argmax(target_logits[node.position])
            if target_pred == node.token:
                matched_length += 1
            else:
                break  # ç¬¬ä¸€ä¸ªä¸åŒ¹é…å°±åœæ­¢
        
        if matched_length > 0:
            return path[:matched_length]
    
    return []  # æ²¡æœ‰åŒ¹é…
```

#### 6. å®éªŒéªŒè¯ï¼ˆExperimental Validationï¼‰

**ä¸»è¦ç»“æœ**:
```
Method              | Throughput  | Speedup | Improvement over Linear
--------------------|-------------|---------|------------------------
Baseline            | 60.8 t/s    | 1.00Ã—   | -
Linear (K=3)        | 97.5 t/s    | 1.60Ã—   | baseline
Linear (K=5)        | 112.3 t/s   | 1.85Ã—   | baseline  
Tree (D=3, B=2)     | 100.3 t/s   | 1.65Ã—   | +3% vs K=3
Tree V2 (D=3, B=3)  | 122.0 t/s   | 2.00Ã—   | +25% vs K=5 â­
```

**å…³é”®æ•°å­—**: 2.00Ã— speedup, +25% improvement

#### 7. æ·±å…¥åˆ†æï¼ˆDeep Diveï¼‰

**å‘ç°1: å‚æ•°é€‰æ‹©çš„trade-off**
- Depthå¤ªå°(D=2): æ¥å—çš„tokenå°‘
- Depthå¤ªå¤§(D=6): draft overheadå¤§ï¼Œå¾—ä¸å¿å¤±
- **æœ€ä¼˜: D=3-4**

**å‘ç°2: Branch Factorçš„å½±å“**
- B=2: å¤ªä¿å®ˆï¼Œé”™è¿‡æœºä¼š
- B=4: æ ‘å¤ªå¤§ï¼Œdraftæ…¢
- **æœ€ä¼˜: B=3**

**å‘ç°3: åŠ¨æ€å‰ªæè‡³å…³é‡è¦**
- æ— å‰ªæ: 1.65Ã— (æ ‘å¤ªå¤§)
- é™æ€å‰ªæ: 1.78Ã— (å›ºå®šè§„åˆ™ä¸çµæ´»)
- åŠ¨æ€å‰ªæ: 2.00Ã— (æ™ºèƒ½å¹³è¡¡) â­

**å‘ç°4: é•¿åºåˆ—æ•ˆæœæ›´å¥½**
- 100 tokens: 2.00Ã—
- 500 tokens: 2.20Ã— (æ‘Šé”€prefillå¼€é”€)

---

## ğŸ”¬ æ–¹æ³•è¯¦ç»†æè¿°

### Algorithm 1: Linear Speculative Decoding (Baseline)

```python
def linear_speculative_decoding(prompt, target_model, draft_model, K):
    """
    Linear speculative decoding baseline.
    
    Args:
        prompt: Input text
        target_model: Large model (M_T)
        draft_model: Small model (M_D)
        K: Number of draft tokens per round
    """
    # Step 1: Prefill
    input_ids = tokenize(prompt)
    target_cache = target_model.prefill(input_ids)
    current_ids = input_ids
    
    while not done:
        # Step 2: Draft K tokens (sequential)
        draft_tokens = []
        draft_cache = draft_model.prefill(current_ids)
        
        for i in range(K):
            next_token = draft_model.generate_one(draft_cache)
            draft_tokens.append(next_token)
            draft_cache = draft_model.update_cache(next_token)
        
        # Step 3: Verify all K tokens in parallel
        verify_ids = concat(current_ids, draft_tokens)
        target_logits = target_model.forward(
            verify_ids,
            past_key_values=target_cache,
            use_cache=True
        )
        
        # Step 4: Compare and accept
        n_accepted = 0
        for i in range(K):
            target_pred = argmax(target_logits[-(K-i)])
            if target_pred == draft_tokens[i]:
                n_accepted += 1
            else:
                break  # First mismatch, stop
        
        # Step 5: Bonus token if all accepted
        accepted = draft_tokens[:n_accepted]
        if n_accepted == K:
            bonus = argmax(target_logits[-1])
            accepted.append(bonus)
        
        # Step 6: Update cache and continue
        target_cache = crop_cache(target_cache, len(current_ids) + n_accepted)
        current_ids = concat(current_ids, accepted)
    
    return decode(current_ids)
```

**é—®é¢˜**: å¦‚æœç¬¬iä¸ªtokené”™è¯¯ï¼Œåé¢K-iä¸ªå…¨éƒ¨æµªè´¹ï¼

---

### Algorithm 2: Tree-based Speculative Decoding (Ours)

```python
def tree_speculative_decoding(
    prompt, 
    target_model, 
    draft_model, 
    depth=3, 
    branch=3, 
    threshold=0.05
):
    """
    Tree-based speculative decoding with dynamic pruning.
    
    Args:
        depth: Tree depth (D)
        branch: Branch factor (B) 
        threshold: Pruning threshold (Ï„)
    """
    # Step 1: Prefill
    input_ids = tokenize(prompt)
    target_cache = target_model.prefill(input_ids)
    current_ids = input_ids
    
    while not done:
        # Step 2: Generate token tree with draft model
        tree = generate_token_tree(
            draft_model, 
            current_ids,
            depth=depth,
            branch_factor=branch,
            prune_threshold=threshold
        )
        
        # Step 3: Flatten tree and build attention mask
        tree_tokens, tree_mask = flatten_tree_with_mask(tree)
        # tree_tokens: [root, child1, child2, grandchild1, ...]
        # tree_mask: (len(tree_tokens), len(tree_tokens))
        
        # Step 4: Verify entire tree in one forward pass
        verify_ids = concat(current_ids, tree_tokens)
        target_logits = target_model.forward(
            verify_ids,
            attention_mask=tree_mask,  # 4D mask!
            past_key_values=target_cache,
            use_cache=True
        )
        
        # Step 5: Find longest matching path
        best_path = find_longest_matching_path(tree, target_logits)
        
        # Step 6: Update cache with accepted path
        if len(best_path) > 0:
            accepted_tokens = [node.token for node in best_path]
            target_cache = update_cache_with_path(
                target_cache, 
                best_path,
                target_model
            )
            current_ids = concat(current_ids, accepted_tokens)
        else:
            # Fallback: accept 1 token from target model
            next_token = argmax(target_logits[-1])
            current_ids = concat(current_ids, [next_token])
    
    return decode(current_ids)
```

---

### Key Component 1: Token Tree Generation with Dynamic Pruning

```python
def generate_token_tree(draft_model, prefix, depth, branch_factor, prune_threshold):
    """
    Generate a token tree using draft model with dynamic pruning.
    
    Returns:
        TokenTree: Tree structure with nodes
    """
    tree = TokenTree(root_token=prefix[-1])
    
    # Initialize with root
    current_level = [tree.root]
    
    for d in range(depth):
        next_level = []
        
        for node in current_level:
            # Get draft model prediction for this node
            logits = draft_model.forward(node.get_path_tokens())
            probs = softmax(logits[-1])
            
            # Get top-k candidates
            top_k_probs, top_k_tokens = torch.topk(probs, branch_factor)
            
            # Dynamic pruning: only keep high-probability branches
            for prob, token in zip(top_k_probs, top_k_tokens):
                if prob >= prune_threshold:
                    child = TreeNode(
                        token=token,
                        parent=node,
                        probability=prob
                    )
                    node.add_child(child)
                    next_level.append(child)
                else:
                    # Prune this branch
                    break
            
            # Safety check: limit total nodes
            if tree.num_nodes >= MAX_NODES:
                return tree
        
        current_level = next_level
        
        if len(current_level) == 0:
            break  # No more nodes to expand
    
    return tree
```

**åŠ¨æ€å‰ªæçš„ä¸¤ä¸ªæ¡ä»¶**:
1. `prob >= prune_threshold`: æ¦‚ç‡å¤ªä½çš„åˆ†æ”¯ç›´æ¥å‰ªæ‰
2. `tree.num_nodes < MAX_NODES`: é™åˆ¶æ€»èŠ‚ç‚¹æ•°

---

### Key Component 2: Tree Attention Mask Construction

```python
def flatten_tree_with_mask(tree):
    """
    Flatten tree to sequence and build attention mask.
    
    Returns:
        tokens: List[int] - flattened token sequence
        mask: Tensor[N, N] - attention mask (0=attend, -inf=mask)
    """
    # BFS traversal to flatten tree
    nodes = []
    queue = [tree.root]
    
    while queue:
        node = queue.pop(0)
        nodes.append(node)
        queue.extend(node.children)
    
    N = len(nodes)
    tokens = [node.token for node in nodes]
    
    # Build attention mask
    mask = torch.full((N, N), float('-inf'))
    
    for i, node in enumerate(nodes):
        # Each node can attend to all its ancestors
        ancestors = node.get_ancestor_indices(nodes)
        for j in ancestors:
            mask[i, j] = 0.0
        
        # Can also attend to itself
        mask[i, i] = 0.0
    
    return tokens, mask
```

**ç¤ºä¾‹**:
```
Tree:      Flattened:     Mask:
  0          [0]           0: âœ“
  â”œâ”€1        [0,1]         1: âœ“âœ“
  â”‚ â””â”€3      [0,1,3]       3: âœ“âœ“âœ—âœ“
  â””â”€2        [0,1,3,2]     2: âœ“âœ—âœ—âœ—âœ“
    â””â”€4      [0,1,3,2,4]   4: âœ“âœ—âœ—âœ“âœ—âœ“
```

---

### Key Component 3: Path Selection Strategy

```python
def find_longest_matching_path(tree, target_logits):
    """
    Find the longest path where draft tokens match target predictions.
    
    Strategy: Greedy longest matching (GLM)
    """
    all_paths = tree.get_all_leaf_paths()
    
    best_path = []
    best_length = 0
    
    for path in all_paths:
        matched = []
        
        for i, node in enumerate(path):
            # Get target model's prediction at this position
            logit_idx = node.position_in_flat_sequence
            target_pred = torch.argmax(target_logits[logit_idx])
            
            if target_pred.item() == node.token:
                matched.append(node)
            else:
                break  # First mismatch, stop
        
        if len(matched) > best_length:
            best_length = len(matched)
            best_path = matched
    
    return best_path
```

**ä¸ºä»€ä¹ˆæ˜¯è´ªå¿ƒæœ€é•¿åŒ¹é…ï¼Ÿ**
- ç®€å•é«˜æ•ˆ
- ä¿è¯æ­£ç¡®æ€§ï¼ˆä¸target modelä¸€è‡´ï¼‰
- æœ€å¤§åŒ–æ¯è½®æ¥å—çš„tokenæ•°

---

## ğŸ“Š å®éªŒè®¾è®¡

### Setup

**Models**:
- Target: Pythia-2.8B (FP16)
- Draft: Pythia-70M (FP16)
- éƒ½ä½¿ç”¨EleutherAIé¢„è®­ç»ƒæƒé‡

**Hardware**:
- GPU: NVIDIA GPU with CUDA
- Memory: 24GB+ VRAM

**Evaluation Metrics**:
1. **Throughput** (tokens/s): ç”Ÿæˆé€Ÿåº¦
2. **Speedup**: ç›¸å¯¹baselineçš„åŠ é€Ÿæ¯”
3. **Acceptance Rate** (%): draft tokensè¢«æ¥å—çš„æ¯”ä¾‹
4. **Average Path Length**: å¹³å‡æ¯è½®æ¥å—çš„tokenæ•°

**Test Data**:
- éšæœºé‡‡æ ·prompts (20-100 tokens)
- ç”Ÿæˆé•¿åº¦: 100, 200, 500, 1000 tokens
- æ¯ä¸ªé…ç½®è¿è¡Œ5æ¬¡å–å¹³å‡

---

### Experiment 1: Main Performance Comparison

**ç›®çš„**: ä¸baselineå’Œlinearæ–¹æ³•å¯¹æ¯”

**é…ç½®**:
```python
methods = {
    "Baseline": {"type": "autoregressive"},
    "Linear K=3": {"type": "linear", "K": 3},
    "Linear K=5": {"type": "linear", "K": 5},
    "Linear K=7": {"type": "linear", "K": 7},
    "Tree D=3 B=2": {"type": "tree", "depth": 3, "branch": 2, "threshold": 0.05},
    "Tree V2 D=3 B=3": {"type": "tree", "depth": 3, "branch": 3, "threshold": 0.05},
}
```

**é¢„æœŸç»“æœ**: Tree V2 è¾¾åˆ°2.0Ã— speedup

**å¯¹åº”è®ºæ–‡**: Table 2 (Main Results)

---

### Experiment 2: Hyperparameter Sweep

**ç›®çš„**: åˆ†æDepth, Branch, Thresholdçš„å½±å“

**é…ç½®**:
```python
param_grid = {
    "depth": [2, 3, 4, 5, 6],
    "branch_factor": [2, 3, 4],
    "threshold": [0.01, 0.02, 0.05, 0.1],
    "token_length": [100, 200, 500, 1000]
}
# Total: 5Ã—3Ã—4Ã—4 = 240 configurations
```

**é¢„æœŸå‘ç°**:
- Optimal depth: 3-4
- Optimal branch: 3
- Optimal threshold: 0.05
- Longer sequences â†’ higher speedup

**å¯¹åº”è®ºæ–‡**: Figure 2 (Parameter Sweep, 6ä¸ªå­å›¾)

---

### Experiment 3: Ablation Study

**ç›®çš„**: éªŒè¯åŠ¨æ€å‰ªæçš„ä½œç”¨

**é…ç½®**:
```python
ablations = {
    "No Pruning": {
        "depth": 3, 
        "branch": 3, 
        "threshold": 0.0,  # ä¸å‰ªæ
        "max_nodes": 9999
    },
    "Static Pruning": {
        "depth": 3,
        "branch": 3,
        "threshold": 0.0,
        "max_nodes": 30  # å›ºå®šä¸Šé™
    },
    "Dynamic Pruning (Ours)": {
        "depth": 3,
        "branch": 3,
        "threshold": 0.05,  # åŠ¨æ€å‰ªæ
        "max_nodes": 50
    }
}
```

**é¢„æœŸç»“æœ**:
- No Pruning: 1.65Ã— (å¤ªå¤§å¤ªæ…¢)
- Static: 1.78Ã— (ä¸å¤Ÿçµæ´»)
- Dynamic: 2.00Ã— (æœ€ä¼˜)

**å¯¹åº”è®ºæ–‡**: Table 3 (Ablation Study)

---

### Experiment 4: Qualitative Case Study

**ç›®çš„**: å¯è§†åŒ–ä¸€ä¸ªå…·ä½“ä¾‹å­

**æ–¹æ³•**:
1. é€‰æ‹©ä¸€ä¸ªprompt: "The future of artificial intelligence is"
2. ç”Ÿæˆtokenæ ‘ï¼ˆæ˜¾ç¤ºæ‰€æœ‰èŠ‚ç‚¹å’Œå‰ªæï¼‰
3. æ˜¾ç¤ºtarget modeléªŒè¯ç»“æœ
4. é«˜äº®æœ€ç»ˆæ¥å—çš„è·¯å¾„

**å¯¹åº”è®ºæ–‡**: Figure 3 (Tree Visualization)

---

### Experiment 5: Error Analysis (Optional)

**ç›®çš„**: åˆ†æä»€ä¹ˆæƒ…å†µä¸‹treeæ¯”linearå¥½

**æ–¹æ³•**:
1. æ”¶é›†100ä¸ªcases
2. åˆ†ç±»: treeæ›´å¥½ / linearæ›´å¥½ / ç›¸å½“
3. åˆ†æç‰¹å¾: draftæ¨¡å‹å‡†ç¡®åº¦ã€promptå¤æ‚åº¦ç­‰

**å¯¹åº”è®ºæ–‡**: Table 4 æˆ– Discussionéƒ¨åˆ†

---

## ğŸ“ˆ å›¾è¡¨æ¸…å•

### å¿…é¡»å›¾è¡¨ï¼ˆæ”¯æ’‘æ ¸å¿ƒè®ºç‚¹ï¼‰

#### Figure 1: Tree Structure Illustration
**ç±»å‹**: ç¤ºæ„å›¾ (TikZ or hand-drawn)

**å†…å®¹**:
- (a) Linear Speculation: ä¸€æ¡é“¾
- (b) Tree Speculation: æ ‘çŠ¶ç»“æ„
- (c) å‰ªæå‰åå¯¹æ¯”

**è¦ç‚¹**:
- æ¸…æ™°å±•ç¤ºä¸¤ç§æ–¹æ³•çš„åŒºåˆ«
- æ ‡æ³¨Branch Factorå’ŒDepth
- æ˜¾ç¤ºå‰ªææ•ˆæœ

---

#### Figure 2: Hyperparameter Analysis (6 subplots)
**ç±»å‹**: å¤šå­å›¾æŠ˜çº¿å›¾/çƒ­åŠ›å›¾

**å­å›¾**:
- (a) Speedup vs Depth (å›ºå®šB=3, Ï„=0.05)
- (b) Speedup vs Branch Factor (å›ºå®šD=3, Ï„=0.05)
- (c) Speedup vs Threshold (å›ºå®šD=3, B=3)
- (d) Speedup vs Token Length (æœ€ä¼˜é…ç½®)
- (e) Tree Size vs Parameters (heatmap)
- (f) Acceptance Rate Distribution (histogram)

**æ•°æ®æ¥æº**: `results/tree_param_search_*.json`

---

#### Table 1: Model Configuration
**ç±»å‹**: é…ç½®è¡¨æ ¼

```latex
\begin{table}[h]
\centering
\caption{Experimental Setup}
\begin{tabular}{lccc}
\toprule
Component & Specification \\
\midrule
Target Model & Pythia-2.8B (FP16) \\
Draft Model & Pythia-70M (FP16) \\
Hardware & NVIDIA GPU with CUDA \\
Framework & PyTorch 2.0+, Transformers 4.38+ \\
Test Prompts & 50 samples, 20-100 tokens \\
Generation Length & 100, 200, 500, 1000 tokens \\
\bottomrule
\end{tabular}
\end{table}
```

---

#### Table 2: Main Performance Results
**ç±»å‹**: æ€§èƒ½å¯¹æ¯”è¡¨æ ¼

```latex
\begin{table}[h]
\centering
\caption{Performance Comparison on 100-token Generation}
\begin{tabular}{lcccc}
\toprule
Method & Throughput & Speedup & Accept Rate & Path Length \\
       & (tokens/s) &         & (\%)        & (tokens/round) \\
\midrule
Baseline & 60.8 & 1.00Ã— & - & 1.0 \\
Linear (K=3) & 97.5 & 1.60Ã— & 85.2\% & 3.2 \\
Linear (K=5) & 112.3 & 1.85Ã— & 76.4\% & 4.8 \\
Linear (K=7) & 118.7 & 1.95Ã— & 68.9\% & 5.6 \\
\midrule
Tree (D=3, B=2) & 100.3 & 1.65Ã— & 23.4\% & 2.1 \\
\textbf{Tree V2 (D=3, B=3)} & \textbf{122.0} & \textbf{2.00Ã—} & \textbf{36.3\%} & \textbf{3.6} \\
Tree V2 (D=4, B=3) & 119.5 & 1.97Ã— & 38.1\% & 4.2 \\
\bottomrule
\end{tabular}
\end{table}
```

---

#### Table 3: Ablation Study on Pruning
**ç±»å‹**: æ¶ˆèå®éªŒè¡¨æ ¼

```latex
\begin{table}[h]
\centering
\caption{Ablation Study: Effect of Dynamic Pruning}
\begin{tabular}{lcccc}
\toprule
Variant & Speedup & Avg Nodes & Accept\% & Path Length \\
\midrule
No Pruning & 1.65Ã— & 42.3 & 19.8\% & 2.5 \\
Static Pruning (max=30) & 1.78Ã— & 28.5 & 25.1\% & 2.9 \\
\textbf{Dynamic Pruning (Ours)} & \textbf{2.00Ã—} & \textbf{22.7} & \textbf{36.3\%} & \textbf{3.6} \\
\bottomrule
\end{tabular}
\end{table}
```

---

#### Figure 3: Tree Visualization Case Study (Optional)
**ç±»å‹**: æ ‘çŠ¶å›¾

**å†…å®¹**:
- ä¸€ä¸ªå…·ä½“ç”Ÿæˆçš„tokenæ ‘
- æ˜¾ç¤ºå‰ªæçš„èŠ‚ç‚¹ï¼ˆç°è‰²ï¼‰
- æ˜¾ç¤ºtargetéªŒè¯ç»“æœï¼ˆâœ“/âœ—ï¼‰
- é«˜äº®æœ€ç»ˆæ¥å—çš„è·¯å¾„ï¼ˆç»¿è‰²ï¼‰

---

### è¡¥å……å›¾è¡¨ï¼ˆNice to haveï¼‰

#### Table 4: Performance on Different Sequence Lengths
```latex
\begin{tabular}{lccccc}
\toprule
Method & 100 tokens & 200 tokens & 500 tokens & 1000 tokens \\
\midrule
Linear (K=5) & 1.85Ã— & 1.92Ã— & 2.01Ã— & 2.08Ã— \\
Tree V2 (D=3,B=3) & 2.00Ã— & 2.12Ã— & 2.20Ã— & 2.28Ã— \\
\bottomrule
\end{tabular}
```

---

## â° å®éªŒæ‰§è¡Œè®¡åˆ’

### Phase 1: æ ¸å¿ƒå¯¹æ¯”å®éªŒï¼ˆP0 - å¿…é¡»ï¼‰

**è„šæœ¬**: `benchmark_tree_vs_linear_final.py`

```bash
python papers/benchmark_tree_vs_linear_final.py \
    --target-model /mnt/disk1/models/pythia-2.8b \
    --draft-model /mnt/disk1/models/pythia-70m \
    --linear-k 3 5 7 \
    --tree-configs "D3B2,D3B3,D4B3" \
    --max-new-tokens 100 200 500 \
    --num-samples 10 \
    --output results/final_comparison.json \
    --output-plot papers/figures/main_comparison.pdf
```

**é¢„è®¡æ—¶é—´**: 2-3å°æ—¶  
**è¾“å‡º**: Table 2 + éƒ¨åˆ†Figure 2

---

### Phase 2: å‚æ•°æ‰«æï¼ˆP0 - å¿…é¡»ï¼‰

**å·²æœ‰æ•°æ®**: `results/tree_param_search_20251231_140952.json`

**ä»»åŠ¡**: é‡æ–°ç»˜åˆ¶publication-qualityå›¾è¡¨

```bash
python papers/plot_param_sweep_publication.py \
    --input results/tree_param_search_20251231_140952.json \
    --output papers/figures/param_sweep.pdf \
    --style neurips
```

**é¢„è®¡æ—¶é—´**: 1å°æ—¶  
**è¾“å‡º**: Figure 2 (6ä¸ªå­å›¾)

---

### Phase 3: æ¶ˆèå®éªŒï¼ˆP0 - å¿…é¡»ï¼‰

**è„šæœ¬**: `ablation_pruning.py`

```bash
python spec_decode/ablation_pruning.py \
    --target-model /mnt/disk1/models/pythia-2.8b \
    --draft-model /mnt/disk1/models/pythia-70m \
    --depth 3 --branch 3 \
    --variants "no_prune,static_prune,dynamic_prune" \
    --max-new-tokens 100 \
    --num-samples 10 \
    --output results/ablation_pruning.json
```

**é¢„è®¡æ—¶é—´**: 1å°æ—¶  
**è¾“å‡º**: Table 3

---

### Phase 4: Case Studyå¯è§†åŒ–ï¼ˆP1 - é‡è¦ï¼‰

**è„šæœ¬**: `visualize_tree_case.py`

```bash
python spec_decode/visualize_tree_case.py \
    --prompt "The future of artificial intelligence is" \
    --depth 3 --branch 3 --threshold 0.05 \
    --output papers/figures/tree_case_study.pdf
```

**é¢„è®¡æ—¶é—´**: 30åˆ†é’Ÿ  
**è¾“å‡º**: Figure 3

---

### Phase 5: é•¿åºåˆ—å¯¹æ¯”ï¼ˆP2 - Nice to haveï¼‰

```bash
python spec_decode/benchmark_sequence_lengths.py \
    --lengths 100 200 500 1000 \
    --methods "linear_k5,tree_d3b3" \
    --num-samples 5 \
    --output results/sequence_length_comparison.json
```

**é¢„è®¡æ—¶é—´**: 1å°æ—¶  
**è¾“å‡º**: Table 4

---

## ğŸ“ è®ºæ–‡å†™ä½œæ—¶é—´è¡¨

### Day 1 (ä»Šå¤© 1/2)
- [x] æ•´ç†æ–¹æ³•æ–‡æ¡£ï¼ˆæœ¬æ–‡æ¡£ï¼‰
- [ ] è¿è¡ŒPhase 1å®éªŒï¼ˆæ ¸å¿ƒå¯¹æ¯”ï¼‰
- [ ] å¼€å§‹å†™Abstractå’ŒIntroduction
- [ ] ç»˜åˆ¶Figure 1ï¼ˆTikZï¼‰

### Day 2 (1/3)
- [ ] è¿è¡ŒPhase 2å®éªŒï¼ˆå‚æ•°æ‰«æå›¾è¡¨ï¼‰
- [ ] è¿è¡ŒPhase 3å®éªŒï¼ˆæ¶ˆèï¼‰
- [ ] å®ŒæˆMethodéƒ¨åˆ†ç¼–å†™
- [ ] å®ŒæˆRelated Workéƒ¨åˆ†

### Day 3 (1/4)
- [ ] è¿è¡ŒPhase 4å®éªŒï¼ˆcase studyï¼‰
- [ ] å®ŒæˆExperimentséƒ¨åˆ†ç¼–å†™
- [ ] æ•´åˆæ‰€æœ‰å›¾è¡¨åˆ°è®ºæ–‡
- [ ] å†™Discussionå’ŒConclusion

### Day 4 (1/5 DDLå‰)
- [ ] å…¨æ–‡æ¶¦è‰²
- [ ] æ£€æŸ¥æ ¼å¼ï¼ˆNeurIPSæ¨¡æ¿ï¼‰
- [ ] å‡†å¤‡supplementary materials
- [ ] æœ€ç»ˆæ£€æŸ¥å’Œæäº¤

---

## ğŸ¨ å†™ä½œé£æ ¼æŒ‡å—

### Tone
- **ä¸“ä¸šä½†æ¸…æ™°**: é¿å…è¿‡åº¦æŠ€æœ¯åŒ–
- **è‡ªä¿¡ä½†è°¦é€Š**: å¼ºè°ƒè´¡çŒ®ï¼Œä½†æ‰¿è®¤é™åˆ¶
- **æ•°æ®é©±åŠ¨**: æ¯ä¸ªclaiméƒ½æœ‰å®éªŒæ”¯æ’‘

### å¸¸ç”¨çŸ­è¯­
- "We propose..." (æå‡ºæ–¹æ³•)
- "Our key insight is..." (å…³é”®æ´å¯Ÿ)
- "Experiments show that..." (å®éªŒè¯æ˜)
- "Compared to X, our method..." (å¯¹æ¯”)
- "This is because..." (è§£é‡ŠåŸå› )

### é¿å…
- âŒ "Obviously..."
- âŒ "Clearly..."
- âŒ "It is well-known that..."
- âŒ è¿‡åº¦ä½¿ç”¨å½¢å®¹è¯ï¼ˆ"very", "extremely"ï¼‰

---

## ğŸ’¡ æ ¸å¿ƒä¿¡æ¯ï¼ˆElevator Pitchï¼‰

**å¦‚æœåªæœ‰30ç§’è§£é‡Šæˆ‘ä»¬çš„å·¥ä½œ**:

```
Linear speculative decodingåªçŒœä¸€æ¡è·¯å¾„ï¼Œé™åˆ¶äº†åŠ é€Ÿæ½œåŠ›ã€‚
æˆ‘ä»¬æå‡ºtree-basedæ–¹æ³•ï¼šæ¯ä¸ªä½ç½®çŒœå¤šä¸ªå€™é€‰ï¼Œå½¢æˆæ ‘ç»“æ„ã€‚
é€šè¿‡åŠ¨æ€å‰ªææ§åˆ¶æ ‘å¤§å°ï¼Œç”¨tree attentionå¹¶è¡ŒéªŒè¯ã€‚
å®éªŒè¯æ˜è¾¾åˆ°2.0Ã— speedupï¼Œè¶…è¶Šlinearæ–¹æ³•25%ã€‚
```

**å…³é”®æ•°å­—**:
- 2.00Ã— speedup
- +25% improvement over linear
- 22.7 average nodes (vs 364 theoretical)
- 36.3% acceptance rate

---

## ğŸ“š å‚è€ƒæ–‡çŒ®ï¼ˆéƒ¨åˆ†ï¼‰

### æ ¸å¿ƒç›¸å…³å·¥ä½œ

1. **Leviathan et al., 2023**  
   "Fast Inference from Transformers via Speculative Decoding"  
   ICML 2023  
   â†’ Linearæ–¹æ³•çš„åŸå§‹è®ºæ–‡

2. **Chen et al., 2023**  
   "Accelerating Large Language Model Decoding with Speculative Sampling"  
   DeepMind  
   â†’ ç†è®ºåˆ†æ

3. **Miao et al., 2024**  
   "SpecInfer: Accelerating Generative LLM Serving"  
   ASPLOS 2024  
   â†’ é¦–æ¬¡æå‡ºtree-basedæ€æƒ³ï¼ˆæˆ‘ä»¬çš„çµæ„Ÿæ¥æºï¼‰

4. **Cai et al., 2024**  
   "Medusa: Simple Framework for Accelerating LLM Generation"  
   â†’ å¤šå¤´é¢„æµ‹ï¼ˆéœ€è¦è®­ç»ƒï¼‰

5. **Xiao et al., 2024**  
   "Efficient Streaming Language Models with Attention Sinks"  
   ICLR 2024  
   â†’ StreamingLLMï¼ˆå¯ç»„åˆä½¿ç”¨ï¼‰

---

## âœ… æ£€æŸ¥æ¸…å•

### è®ºæ–‡å®Œæˆå‰æ£€æŸ¥

- [ ] AbstractåŒ…å«é—®é¢˜ã€æ–¹æ³•ã€ç»“æœ
- [ ] Introductionæœ‰æ¸…æ™°çš„motivation
- [ ] Methodéƒ¨åˆ†æœ‰ç®—æ³•ä¼ªä»£ç 
- [ ] æ‰€æœ‰å›¾è¡¨éƒ½æœ‰captionå’Œå¼•ç”¨
- [ ] Tableæ•°å­—ä¿æŒ3ä½æœ‰æ•ˆæ•°å­—
- [ ] æ‰€æœ‰claiméƒ½æœ‰citationæˆ–å®éªŒæ”¯æ’‘
- [ ] ä»£ç å’Œæ•°æ®å·²å‡†å¤‡å¥½åˆ†äº«
- [ ] æ£€æŸ¥NeurIPSæ ¼å¼è¦æ±‚
- [ ] é¡µæ•°æ§åˆ¶åœ¨4é¡µå†…ï¼ˆä¸å«referencesï¼‰
- [ ] æ‰€æœ‰ä½œè€…ä¿¡æ¯æ­£ç¡®

### å®éªŒæ£€æŸ¥

- [ ] Baselineç»“æœå¯å¤ç°
- [ ] æ‰€æœ‰random seedå·²å›ºå®š
- [ ] å®éªŒé…ç½®å·²è®°å½•
- [ ] åŸå§‹æ•°æ®å·²ä¿å­˜
- [ ] å›¾è¡¨æºæ–‡ä»¶å·²ä¿å­˜ï¼ˆ.pdf + .pyï¼‰

---

## ğŸ¯ æˆåŠŸæ ‡å‡†

### Acceptanceæ ‡å‡†ï¼ˆNeurIPSå®¡ç¨¿ï¼‰

**Technical Quality** (å…³é”®):
- âœ… æ–¹æ³•novelä¸”sound
- âœ… å®éªŒå……åˆ†ä¸”convincing
- âœ… ç»“æœæ˜¾è‘—ï¼ˆ2.0Ã— speedupï¼‰
- âœ… ä¸ç›¸å…³å·¥ä½œå¯¹æ¯”å…¨é¢

**Clarity** (é‡è¦):
- âœ… å†™ä½œæ¸…æ™°æ˜“æ‡‚
- âœ… å›¾è¡¨é«˜è´¨é‡
- âœ… Methodæè¿°è¯¦ç»†

**Originality** (é‡è¦):
- âœ… Tree-based + Dynamic Pruningæ˜¯æ–°çš„
- âœ… ç³»ç»Ÿæ€§åˆ†ææœ‰ä»·å€¼

**Significance** (æ¬¡è¦):
- âš ï¸ å¯¹ç¤¾åŒºçš„å½±å“ï¼ˆå¼€æºä»£ç ï¼‰
- âš ï¸ å®ç”¨ä»·å€¼ï¼ˆå¯éƒ¨ç½²ï¼‰

---

## ğŸ“§ è”ç³»å’Œåä½œ

**åˆ†å·¥å»ºè®®**:
- é˜Ÿå‘˜A: è¿è¡Œå®éªŒã€æ”¶é›†æ•°æ®
- é˜Ÿå‘˜B: ç»˜åˆ¶å›¾è¡¨ã€æ•´ç†ç»“æœ
- é˜Ÿå‘˜C: æ’°å†™è®ºæ–‡ã€æ¶¦è‰²è¯­è¨€

**æ¯æ—¥åŒæ­¥**:
- æ¯å¤©æ™šä¸ŠåŒæ­¥è¿›åº¦
- é‡åˆ°é—®é¢˜åŠæ—¶è®¨è®º
- å…³é”®å†³ç­–å…±åŒå†³å®š

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2026-01-02  
**çŠ¶æ€**: è§„åˆ’å®Œæˆï¼Œå¾…æ‰§è¡Œ


