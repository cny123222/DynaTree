# å®Œæ•´æ¶ˆèå®éªŒæ•°æ®åˆ†æ

## âœ… **æ•°æ®é½å…¨ï¼**

æ‰€æœ‰æ•°æ®æ¥æºäºWikiText-2æ•°æ®é›†ï¼Œ500 tokensç”Ÿæˆï¼š

### **æ•°æ®æ¥æº**

1. **Baseline & Linear K=6**: `results/ä¸åŒç”Ÿæˆtokené•¿åº¦æ€§èƒ½å¯¹æ¯”/wikitext_benchmark_500tokens.json`
2. **Tree (æ— å‰ªæ & æœ‰å‰ªæ)**: `results/å‰ªææ¶ˆèå®éªŒç»“æœ.json`

### **Baselineä¸€è‡´æ€§**
- WikiText benchmark: 127.9 t/s
- å‰ªææ¶ˆèå®éªŒ: 128.6 t/s
- å·®å¼‚: 0.7 t/s (0.5%) âœ… å¯æ¥å—

---

## ğŸ“Š **å®Œæ•´æ¶ˆèå®éªŒæ•°æ®ï¼ˆ4æ­¥ï¼‰**

ä½¿ç”¨å‰ªææ¶ˆèå®éªŒçš„Baseline (128.6 t/s) ä½œä¸ºåŸºå‡†ï¼š

### **Step 1: Baseline (AR only)**
```
Throughput: 128.6 t/s
Speedup: 1.00Ã—
```
- çº¯è‡ªå›å½’è§£ç 
- æ— draft modelï¼Œæ— speculation

### **Step 2: + Draft Model (Linear K=6)**
```
Throughput: 174.2 t/s
Speedup: 1.35Ã—
Gain: +35.5%
```
- æ·»åŠ draft modelå’Œçº¿æ€§æŠ•æœºè§£ç 
- **è´¡çŒ®æœ€å¤§çš„ç»„ä»¶**

### **Step 3: + Tree Structure (D=7, B=2, æ— å‰ªæ t=0.0)**
```
Throughput: 63.4 t/s
Speedup: 0.49Ã—
Gain over Linear: -63.6% âš ï¸
```
- æ·»åŠ æ ‘å½¢ç»“æ„ï¼Œä½†**æ— å‰ªæ**
- **æ€§èƒ½å¤§å¹…ä¸‹é™ï¼**
- åŸå› ï¼šæ ‘å¤ªå¤§ï¼ŒéªŒè¯å¼€é”€è¿‡é«˜

### **Step 4: + Adaptive Pruning (Ï„=0.05)**
```
Throughput: 182.7 t/s
Speedup: 1.42Ã—
Gain over no-prune: +188%
Total gain over baseline: +42.1%
```
- æ·»åŠ è‡ªé€‚åº”å‰ªæ
- **å‰ªææ˜¯å…³é”®ï¼**
- æœ€ç»ˆæ€§èƒ½è¶…è¶ŠLinear

---

## âš ï¸ **å…³é”®å‘ç°**

### **Tree without pruning is SLOWER than Linear!**

```
Linear K=6:          174.2 t/s (1.35Ã—)
Tree (no prune):     63.4 t/s (0.49Ã—)  âŒ æ¯”Baselineè¿˜æ…¢ï¼
Tree (with prune):   182.7 t/s (1.42Ã—)  âœ… æœ€å¿«
```

**åŸå› åˆ†æ**ï¼š
1. **Tree D=7, B=2, æ— å‰ªæ** ä¼šç”Ÿæˆéå¸¸å¤§çš„æ ‘
   - ç†è®ºèŠ‚ç‚¹æ•°ï¼š1 + 2 + 4 + 8 + 16 + 32 + 64 = 127ä¸ªèŠ‚ç‚¹
   - å®é™…å¯èƒ½æ›´å°‘ï¼ˆå› ä¸ºæœ‰äº›è·¯å¾„çŸ­ï¼‰ï¼Œä½†ä»ç„¶å¾ˆå¤§

2. **å¤§æ ‘çš„éªŒè¯å¼€é”€éå¸¸é«˜**
   - éœ€è¦å¯¹æ‰€æœ‰èŠ‚ç‚¹åšforward pass
   - å†…å­˜å ç”¨å¤§
   - è®¡ç®—å¼€é”€å¤§

3. **Acceptance rateä¹Ÿä½**
   - æ— å‰ªæï¼š60.4%
   - æœ‰å‰ªæï¼š72.6%
   - è¯´æ˜å‰ªæåçš„æ ‘è´¨é‡æ›´å¥½

---

## ğŸ¯ **ä¸¤ç§æ¶ˆèå®éªŒæ–¹æ¡ˆ**

### **æ–¹æ¡ˆ A: æ ‡å‡†4æ­¥æ¶ˆèï¼ˆå±•ç¤ºå‰ªæçš„å…³é”®æ€§ï¼‰** â­ æ¨è

```
Step 1: Baseline               128.6 t/s (1.00Ã—)
Step 2: + Draft (Linear)       174.2 t/s (1.35Ã—) [+35%]
Step 3: + Tree (no prune)      63.4 t/s (0.49Ã—)  [-64%] âš ï¸
Step 4: + Adaptive Pruning     182.7 t/s (1.42Ã—) [+188%]
```

**ä¼˜ç‚¹**ï¼š
- âœ… å±•ç¤ºå‰ªæçš„**å…³é”®é‡è¦æ€§**
- âœ… è¯´æ˜ä¸ºä»€ä¹ˆpruningæ˜¯æ ¸å¿ƒè´¡çŒ®
- âœ… æ‰€æœ‰æ•°æ®100%çœŸå®

**ç¼ºç‚¹**ï¼š
- âš ï¸ Step 3æ€§èƒ½ä¸‹é™ï¼Œéœ€è¦è§£é‡Š
- âš ï¸ ä¸æ˜¯"å•è°ƒé€’å¢"çš„æ¶ˆèå®éªŒ

**æ–‡å­—è¯´æ˜**ï¼š
```
Table X shows a 4-step ablation study. Starting from autoregressive 
decoding (128.6 t/s), adding a draft model yields 35% improvement 
(174.2 t/s). However, naively expanding to a tree structure **without 
pruning** (t=0.0) significantly degrades performance (63.4 t/s, 0.49Ã—), 
as the large unpruned tree introduces excessive verification overhead. 
This demonstrates that **adaptive pruning is essential** for tree-based 
speculation. With pruning (t=0.05), DynaTree achieves 182.7 t/s (1.42Ã—), 
a 188% improvement over the unpruned tree and 5% faster than linear 
speculation, validating the effectiveness of probability-based branch 
pruning.
```

---

### **æ–¹æ¡ˆ B: ç®€åŒ–3æ­¥æ¶ˆèï¼ˆè·³è¿‡æ— å‰ªææ­¥éª¤ï¼‰**

```
Step 1: Baseline               128.6 t/s (1.00Ã—)
Step 2: + Draft (Linear)       174.2 t/s (1.35Ã—) [+35%]
Step 3: + Tree + Pruning       182.7 t/s (1.42Ã—) [+5%]
```

**ä¼˜ç‚¹**ï¼š
- âœ… å•è°ƒé€’å¢ï¼Œé€»è¾‘æ¸…æ™°
- âœ… é¿å…è§£é‡Š"ä¸ºä»€ä¹ˆStep 3å˜æ…¢"

**ç¼ºç‚¹**ï¼š
- âŒ æ— æ³•çªå‡ºå‰ªæçš„é‡è¦æ€§
- âŒ Treeå’ŒPruningåˆå¹¶ï¼Œè´¡çŒ®ä¸æ˜ç¡®

---

## ğŸ“Š **æ¨èï¼šæ–¹æ¡ˆAï¼ˆ4æ­¥æ¶ˆèï¼‰**

### **ä¸ºä»€ä¹ˆæ¨èæ–¹æ¡ˆAï¼Ÿ**

1. **çªå‡ºæ ¸å¿ƒè´¡çŒ®**: è¯æ˜adaptive pruningæ˜¯DynaTreeçš„å…³é”®åˆ›æ–°
2. **çœŸå®åæ˜ ç ”ç©¶è¿‡ç¨‹**: ç›´æ¥å±•ç¤º"ä¸ºä»€ä¹ˆéœ€è¦pruning"
3. **å¢å¼ºè®ºæ–‡å¯ä¿¡åº¦**: ä¸éšè—è´Ÿé¢ç»“æœï¼Œåè€Œæ›´æœ‰è¯´æœåŠ›
4. **å›ç­”å®¡ç¨¿äººé—®é¢˜**: å¦‚æœå®¡ç¨¿äººé—®"ä¸ºä»€ä¹ˆéœ€è¦pruning"ï¼Œè¿™ä¸ªæ•°æ®å°±æ˜¯ç­”æ¡ˆ

### **LaTeXè¡¨æ ¼ï¼ˆæ–¹æ¡ˆAï¼‰**

```latex
\begin{table}[t]
\centering
\caption{\textbf{Ablation study: progressive component addition.} Starting from autoregressive decoding, we incrementally add (i)~draft-based speculation, (ii)~tree structure, and (iii)~adaptive pruning. Notably, an unpruned tree (Step~3) severely degrades performance due to excessive verification overhead, demonstrating that probability-based pruning is essential for efficient tree-based speculation.}
\label{tab:ablation}
\begin{tabular}{llccc}
\toprule
Step & Components & Configuration & Throughput & Speedup \\
\midrule
1 & Baseline & AR only & 128.6 & 1.00\(\times\) \\
2 & + Draft model & Linear K=6 & 174.2 & 1.35\(\times\) \\
3 & + Tree structure & D=7, B=2, \(\tau\)=0.0 & 63.4 & 0.49\(\times\) \\
\textbf{4} & \textbf{+ Adaptive pruning} & \(\tau\)=\textbf{0.05} & \textbf{182.7} & \textbf{1.42\(\times\)} \\
\bottomrule
\end{tabular}
\end{table}
```

### **æ–‡å­—è¯´æ˜ï¼ˆæ­£æ–‡ï¼‰**

```latex
\subsection{Ablation Study}

To isolate the contribution of each algorithmic component, we conduct a 
4-step ablation study by progressively adding features to the baseline 
autoregressive decoder. Table~\ref{tab:ablation} summarizes the results 
on WikiText-2 generating 500 tokens. Starting from pure autoregressive 
generation (128.6 tokens/s), introducing speculative decoding with a draft 
model (Linear K=6) yields a 35\% improvement (174.2 tokens/s), demonstrating 
the core benefit of parallel verification.

Expanding to a tree structure without pruning ($\tau$=0.0, Step 3), however, 
\emph{severely degrades performance} to 63.4 tokens/s (0.49$\times$ speedup). 
This counterintuitive result occurs because an unpruned tree at depth 7 
generates up to 127 nodes, creating excessive verification overhead that 
outweighs the benefits of multi-path exploration. The low acceptance rate 
(60.4\%) further indicates that many low-probability branches waste computation.

Adding adaptive pruning ($\tau$=0.05, Step 4) recovers and exceeds the 
performance, achieving 182.7 tokens/s (1.42$\times$ speedup), a \textbf{188\% 
improvement} over the unpruned tree. This dramatic recovery demonstrates that 
\textbf{probability-based pruning is essential} for tree-based speculative 
decoding. By dynamically removing low-probability branches, pruning reduces 
the average tree size while maintaining high-quality paths, resulting in 
5\% faster throughput than linear speculation.

The ablation study confirms that both draft-based speculation (+35\%) and 
adaptive pruning (+188\% over unpruned) contribute significantly to DynaTree's 
final performance, with pruning being the critical factor that makes tree-based 
approaches practical.
```

---

## ğŸ“ˆ **å¯è§†åŒ–å»ºè®®**

### **æŸ±çŠ¶å›¾ï¼ˆæ˜¾ç¤ºè´Ÿå¢é•¿ï¼‰**

```
     200â”¤                              â•­â”€â”€â”€â”€â”€â•®
        â”‚                              â”‚     â”‚
     150â”¤             â•­â”€â”€â”€â”€â”€â•®          â”‚ 1.42â”‚
        â”‚             â”‚     â”‚          â”‚     â”‚
     100â”¤   â•­â”€â”€â”€â”€â”€â•®   â”‚ 1.35â”‚   â•­â”€â”   â•°â”€â”€â”€â”€â”€â•¯
        â”‚   â”‚     â”‚   â”‚     â”‚   â”‚ â”‚
      50â”¤   â”‚ 1.00â”‚   â•°â”€â”€â”€â”€â”€â•¯   â”‚0.49
        â”‚   â”‚     â”‚              â”‚ â”‚
       0â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           Step1   Step2        Step3   Step4
```

**å…³é”®**ï¼š
- Step 3çš„æŸ±å­æ˜æ˜¾ä½äºStep 2
- ç”¨ä¸åŒé¢œè‰²æ ‡æ³¨ï¼ˆå¦‚çº¢è‰²ï¼‰
- ç®­å¤´æˆ–æ ‡æ³¨è¯´æ˜"Pruning is essential"

---

## ğŸ¯ **ä¸‹ä¸€æ­¥è¡ŒåŠ¨**

### **Option 1: ç«‹å³æ·»åŠ 4æ­¥æ¶ˆèå®éªŒ** â­ æ¨è

æˆ‘å¯ä»¥å¸®ä½ ï¼š
1. åˆ›å»ºç»˜å›¾è„šæœ¬ (`plot_ablation_4steps.py`)
2. ç”Ÿæˆæ¶ˆèå›¾è¡¨ (æ˜¾ç¤ºè´Ÿå¢é•¿)
3. æ›´æ–°LaTeXè¡¨æ ¼å’Œæ–‡å­—
4. é‡æ–°ç¼–è¯‘PDF

é¢„è®¡æ—¶é—´ï¼š30åˆ†é’Ÿ

### **Option 2: ä½¿ç”¨ç®€åŒ–çš„3æ­¥æ¶ˆè**

è·³è¿‡"æ— å‰ªæ"æ­¥éª¤ï¼Œåªå±•ç¤ºï¼š
- Baseline â†’ Linear â†’ Tree (pruned)

é¢„è®¡æ—¶é—´ï¼š20åˆ†é’Ÿ

---

## ğŸ’¡ **æˆ‘çš„å»ºè®®**

**é€‰æ‹©æ–¹æ¡ˆAï¼ˆ4æ­¥æ¶ˆèï¼‰**ï¼Œå› ä¸ºï¼š

1. **çªå‡ºæ ¸å¿ƒè´¡çŒ®**: Adaptive pruningæ˜¯ä½ ä»¬çš„å…³é”®åˆ›æ–°
2. **å›ç­”å®¡ç¨¿äººè´¨ç–‘**: è¯æ˜pruningä¸æ˜¯"é”¦ä¸Šæ·»èŠ±"ï¼Œè€Œæ˜¯"å¿…ä¸å¯å°‘"
3. **å¢å¼ºå¯ä¿¡åº¦**: ä¸éšè—è´Ÿé¢ç»“æœï¼Œåè€Œæ˜¾å¾—æ›´è¯šå®
4. **æ•…äº‹æ€§å¼º**: "We tried tree structure, it failed, then we added pruning, it succeeded"

è¿™æ˜¯ä¸€ä¸ª**éå¸¸æœ‰è¯´æœåŠ›**çš„æ¶ˆèå®éªŒï¼

---

**ä½ æƒ³é€‰å“ªä¸ªæ–¹æ¡ˆï¼Ÿå‘Šè¯‰æˆ‘ï¼Œæˆ‘ç«‹å³å¼€å§‹å®ç°ï¼**

